[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cfRNA POP Study",
    "section": "",
    "text": "Introduction\nThis is a website to supplement the following paper by Gong et al. submitted to STM: Elevated levels of circulating Leptin (LEP) and Pappalysin2 (PAPPA2) cell-free RNAs are the hallmarks of pregnancies complicated by preeclampsia combined with fetal growth restriction.\nIt contains the R and other relevant codes (e.g. bash or graphviz) that were used to process the RNA-seq data and to generate the main and the supplementary figures in the paper.\nThe aim of this study was to identify cell-free RNA (cfRNA) transcripts circulating in the maternal blood that are predictive of pregnancies in combination of fetal growth restriction (FGR) and preeclampsia (PE).\n\n\n\n\n\n\nFigure 1: Schematic diagrams showing the current study design\n\n\n\nThe maternal blood was drawn at around 12, 20, 28 and 36 weeks of gestational age (wkGA) and 2ml of plasma from each of the blood sample was used to extract RNA. A total of 751 maternal plasma samples from 195 pregnant women (39 cases; 156 non-cases) were collected and samples from a case subject at a given gestational age were analysed in the same batch as the matched controls which were also obtained at the same gestational age.\nWe divided our cohort into discovery and validation groups. The discovery group consisted of 15 PE with FGR cases resulting in preterm delivery (&lt;37 week of gestational age) and the validation group consisted of 24 PE with FGR resulting in term delivery. Each case was paired with ~4 matched controls hence a total of 60 and 96 healthy control samples, with no overlap, were included in the discovery and the validation group, respectively.\nAt the discovery stage, cfRNAs that were differentially expressed in cases compared to controls were detected at each of the gestational age group and 11 machine learning (ML) methods (Figure 3.1) were compared to find the best predictive models by controlling the number of predictor mRNAs. The best performing model from the discovery stage was validated across each gestational age group in our internal validation dataset. We also analysed the external validation dataset which reported cfRNAs in plasma samples from women with an established diagnosis of PE.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "sample.html",
    "href": "sample.html",
    "title": "1  Sample Information",
    "section": "",
    "text": "1.1 Discovery Cohort (preterm dataset)\nListing 1.1: Tabulate the discovery dataset\n\n\nxtabs(~GA+Condition, dt.samples[Cohort==\"preterm\"]) %&gt;% addmargins\n\n\n\n\n      Condition\nGA     Case Control Sum\n  12wk   15      60  75\n  20wk   14      60  74\n  28wk   13      60  73\n  36wk    1      56  57\n  Sum    43     236 279",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sample Information</span>"
    ]
  },
  {
    "objectID": "sample.html#validation-cohort-term-dataset",
    "href": "sample.html#validation-cohort-term-dataset",
    "title": "1  Sample Information",
    "section": "1.2 Validation Cohort (term dataset)",
    "text": "1.2 Validation Cohort (term dataset)\n\n\n\n\nListing 1.2: Tabulate the validation dataset\n\n\nxtabs(~GA+Condition, dt.samples[Cohort==\"term\"]) %&gt;% addmargins\n\n\n\n\n      Condition\nGA     Case Control Sum\n  12wk   24      96 120\n  20wk   22      96 118\n  28wk   24      97 121\n  36wk   21      96 117\n  Sum    91     385 476",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sample Information</span>"
    ]
  },
  {
    "objectID": "quant.html",
    "href": "quant.html",
    "title": "2  RNA-seq data processing",
    "section": "",
    "text": "2.1 Transcript quantification\nFor an RNA-seq quantification method applied in this study, we chose the most reliable approach based on the performance of predicting fetal sex by measuring the extent of chromosome Y encoded transcripts (see Chapter 4 for detail). Based on this benchmark result, we chose Salmon (v1.5.2) in mapping-mode to process our RNA-seq datasets.",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>RNA-seq data processing</span>"
    ]
  },
  {
    "objectID": "quant.html#sec-mtd-rna-seq-quant",
    "href": "quant.html#sec-mtd-rna-seq-quant",
    "title": "2  RNA-seq data processing",
    "section": "",
    "text": "2.1.1 Salmon index\n\n\n\nListing 2.1: A pseudo-code to run `salmon index`\n\n\n\n\nstatic/shell/salmon.decoy.sh\n\n$HOME/Install/SalmonTools/scripts/generateDecoyTranscriptome.sh \\\n  -j 32 \\\n  -m $HOME/Install/mashmap/mashmap \\\n  -g $HOME/data/genome/Homo_sapiens/Ensembl/GRCh38/Sequence/WholeGenomeFasta/genome.fa \\\n  -a $HOME/results/RNA-Seq/Placentome/gffcompare/POPS-2022/POPS-Placenta-Transcriptome/POPS-2022.GRCh38.88.Novel.Known.Freq.0.1.TPM.0.1.tr.reconstruction.gffread.gtf \\\n  -t $HOME/results/RNA-Seq/Placentome/gffcompare/POPS-2022/POPS-Placenta-Transcriptome/POPS-2022.GRCh38.88.Novel.Known.Freq.0.1.TPM.0.1.tr.reconstruction.gffread.gtf.fa \\\n  -o $HOME/data/Salmon/decoy/Homo_sapiens/Ensembl/GRCh38/POPS-2022.GRCh38.88.Novel.Known.Freq.0.1.TPM.0.1.tr.reconstruction\n\n\n\n\n\n\n2.1.2 Salmon quant\n\n\n\nListing 2.2: A pseudo-code to run `salmon quant` in mapping mode\n\n\n\n\nstatic/shell/salmon-quant.sh\n\nsalmon quant -p 32 \\\n             -i POPS_TR_INDEX  \\\n             -l A \\\n             -1 S1_FQ_1.fq \\\n             -2 S1_FQ_2.fq \\\n             --seqBias \\\n             --gcBias \\\n             --posBias \\\n             --discardOrphanQuasi \\\n             --writeUnmappedNames \\\n             --writeMapping \\\n             -o OUTPUT_DIR | samtools view -bS &gt; OUTPUT_DIR/S1_salmon.bam",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>RNA-seq data processing</span>"
    ]
  },
  {
    "objectID": "quant.html#sec-mtd-deg",
    "href": "quant.html#sec-mtd-deg",
    "title": "2  RNA-seq data processing",
    "section": "2.2 Differentially expressed gene analysis",
    "text": "2.2 Differentially expressed gene analysis\nThe differentially expressed gene analysis was conducted for each gestational epoch (12wk, 20wk, 28wk, and 36wk) separately, except for the 36wkGA gestation samples of the pre-term dataset which has only one PE case.\nFirstly, the following parameters were defined:\n\n\n\nListing 2.3: Set the parameters\n\n\n\n\nstatic/R/config/cfRNA-2024.R\n\n# set parameters\nmy.disease=\"PE\"\nmy.type=\"preterm\"\nmy.salmon=\"Salmon\"\nmy.salmon.index=\"POPS-2022.GRCh38.88\"\nmy.slx&lt;-\"SLX-ILM-Plasma2021.Homo_sapiens.v1\"\n\n# set filter\nminCPM=0.1; minRead=10; minFreq=0.1; minFC=1.2\n\n# which p.adjust methods?\nadjust.methods=c(`Benjamini & Yekutieli`=\"BY\",\n            `Benjamini & Hochberg`=\"BH\",\n            `Bonferroni`=\"bonferroni\")\n\n\n\n\nThe transcript-level read count matrices (e.g. quant.sf files) were imported using tximeta (v1.8.5) Bioconductor package and merged at the gene-level.\n\n\n\n\nListing 2.4: Code to set the Salmon quant files of the pre-term dataset (i.e. Discovery dataset)\n\n\ndt.foo&lt;-data.table(`files`=system(paste0(\"ls \", \"~/results/\",my.slx,\"/\",my.salmon,\"/\",my.salmon.index,\"/*/quant.sf\"), intern=T))\ndt.foo[,names:=tstrsplit(files,\"/\",keep=8)]\ndt.colDataAll&lt;-merge(dt.foo,dt.samples,by.x=\"names\",by.y=\"SampleID\") # n=755\n\n# pre-term\ndt.colData&lt;-dt.colDataAll[Type==my.type] # n=279\n#dt.colData[grepl(\"-b$\",names)] # CX (CX-b): both failed; HQ (HQ-b): only HQ-b passed QC according to illumina\ndt.colData&lt;-dt.colData[!names %in% c(\"GS-59-CX-b\",\"GS-179-HQ\")] # remove these two samples (n=277)\nli.GA&lt;-split(dt.colData, dt.colData$GA)\n\n#tx2gene\ndt.tx2gene&lt;-fread(\"~/results/RNA-Seq/Placentome/gffcompare/POPS-2022/POPS-Placenta-Transcriptome/POPS-2022.GRCh38.88.Novel.Known.Freq.0.1.TPM.0.1.tr.reconstruction.tx2gene.txt\", header=F,col.names=c(\"transcript_id\",\"gene_id\"))\n\n\n\n\n\n\n\n\nListing 2.5: Code to make dds (DESeq Data Set) object\n\n\nlibrary(DESeq2)\nlibrary(edgeR)\nlibrary(\"BiocParallel\")\nregister(MulticoreParam(12))\n\n#tximeta via tx2gene\n# 36wk dataset excluded from the beginnin\ngse&lt;-tximeta::tximeta(dt.colData[GA!=\"36wk\"],\n                      skipMeta=T,\n                      tx2gene=dt.tx2gene[,.(transcript_id,gene_id)],txOut=F)\n\n# set up `dds` at gene-level\nmy.design &lt;- formula(~ Batch + GA + Sex + Condition)    # isa 'formula'\ndds &lt;- DESeqDataSet(se=gse, design=my.design)\n\ndds$Group&lt;-factor(paste0(dds$GA,dds$Condition)) # add 'Group'\ndesign(dds) &lt;- formula(~ Batch + Sex + Group)     # isa 'formula'\n\ndds&lt;- DESeq(dds, parallel=TRUE) # isa 'DESeqDataSet'\n\n\n\n\n\n2.2.1 DESeq2\nWe only considered genes found in ≥10% of samples (i.e. ≥ 23) having ≥10 reads, and discarded genes detected as dispersion outliers by DESeq2 (v1.30.0).\n\n\n\n\nListing 2.6: Code to set up dds object\n\n\n# at leat 10 reads for at leat 10  % of the samples \nkeep &lt;- rowSums(counts(dds) &gt;= minRead) &gt;= ncol(dds)*minFreq \ndds.f&lt;- DESeq(dds[keep,], parallel=TRUE) # isa 'DESeqDataSet'\ndim(dds.f) # 15380 x 221\n\n\n\n\nA total of 15,150 genes and 221 samples were used to find differentially expressed genes by adjusting the batch number, the fetal sex, and the gestion of the samples in the design matrix of DESeq2. The p-values were calculated from the null hypothesis that the fold changes were less than or equal to 20% (i.e. lfcThreshold=log2(1.2)) in case and control groups.\n\n\n\n\nListing 2.7: Code to find DEGs by DESeq\n\n\n# remove dispOutlier genes\nkeep2&lt;-!is.na(rowData(dds.f)[,\"dispOutlier\"]) & !rowData(dds.f)[,\"dispOutlier\"]\ndds.f2&lt;- DESeq(dds.f[keep2], parallel=T)\n\ndim(dds.f2) # 15150 x 221\n\n# apply shink separately for 12wk, 20wk, and 28wk (li.GA[1:3])\nli.resLFC&lt;-lapply(names(li.GA)[1:3],function(my.GA){ \n  my.res&lt;-results(dds.f2, \n          independentFiltering=FALSE, #by default independant filtering at FDR (alpha) 0.1 (10%) \n          lfcThreshold=log2(minFC), \n          contrast=c(\"Group\",paste0(my.GA,c(\"Case\",\"Control\"))),\n          parallel=TRUE)  \n  lfcShrink(dds.f2, \n            res=my.res,\n            lfcThreshold=log2(minFC), # not applicable for 'asher' \n            type=\"ashr\",\n            parallel=TRUE)  \n})\nnames(li.resLFC)&lt;-names(li.GA)[1:3]\n\ndl.resLFC&lt;-lapply(li.resLFC, function(i)\n  data.table(`gene_id`=rownames(i), \n              as.data.frame(i))[order(pvalue)][,`:=`(\"BH\"=p.adjust(pvalue,\"BH\"),\n                                              \"BY\"=p.adjust(pvalue,\"BY\"),\n                                              \"bf\"=p.adjust(pvalue,\"bonferroni\"))]\n)\nfwrite(dl.resLFC[[\"28wk\"]], file=paste0(\"static/R/result/DEG.DSeq2.28wk.\",my.type,\".\",my.salmon.index,\".csv\"))\n\n\n\n\n\n\n2.2.2 edgeR\nFor edgeR (v3.32.1) analysis, we used makeDGEList function of tximeta Bioconductor package to convert the data object of the 15,150 genes across the 221 samples as mentioned above. The gene-level count matrix was normalised by using calcNormFactors function of edgeR with TMM method (trimmed mean of M values) and a quasi-likelihood negative binomial generalised log-linear model (i.e. glmQLFit) was applied to account for the batch number, the fetal sex and the gestation information in the design matrix of edgeR. For a statistical test, we used glmTreat of edgeR with at least 20% fold-change (i.e. lfc=log2(1.2)).\n\n\n\n\nListing 2.8: Code to find DEGs by edgeR\n\n\ngse2 &lt;-gse[rownames(dds.f2),]\nd2&lt;-tximeta::makeDGEList(gse2)\nd2$samples&lt;-cbind(d2$samples,colData(gse2))\nd2$samples$group&lt;-factor(paste0(d2$samples$GA,d2$samples$Condition)) # add 'group'\nd2$samples$GA&lt;-droplevels(d2$samples$GA) # 36wk removed\nd2$samples$Batch&lt;-droplevels(d2$samples$Batch) # some batches removed\n\n# additional filter\n#keep &lt;- filterByExpr(d2)\n#keep %&gt;% table # FALSE:13, TRUE:15137\n#d2&lt;-d2[keep,]\n\n# TMM normalisation (default). It adds `norm.factors` d2$samples\n# NB, we have `offsets`, which take precedence over lib.size and norm.factors\nd2&lt;-calcNormFactors(d2,method=\"TMM\") \n                                    \nplotMDS(d2, col=as.numeric(d2$samples$GA))\nplotMDS(d2, col=as.numeric(d2$samples$Condition))\n\n# design\nmy.design &lt;- model.matrix(~ 0 + group + Sex + Batch , data=d2$samples)      # isa 'matrix'\n(my.contrasts &lt;- makeContrasts(`12wk`=group12wkCase-group12wkControl, \n                              `20wk`=group20wkCase-group20wkControl, \n                              `28wk`=group28wkCase-group28wkControl, \n                              levels=my.design)\n)\n\n# dispersion\n#dp2 = estimateDisp(d2, design=my.design, verbose=T)\ndp2 = estimateGLMCommonDisp(d2, design=my.design, verbose=T) \ndp2 = estimateGLMTrendedDisp(dp2, design=my.design, verbose=T)\ndp2 = estimateGLMTagwiseDisp(dp2, design=my.design)\n\nplotMDS(dp2)\nplotBCV(dp2)\n\n# fit\n#f = glmFit(dp2, design=my.design) # \nf = glmQLFit(dp2, design=my.design) # QL(Quasi-like) pipeline\n\nplotMD(f)\nplotQLDisp(f)\n\n# get the edgeR results\nli.res.edgeR&lt;-lapply(names(li.GA)[1:3], function(i){\n  te &lt;- glmTreat(f, contrast=my.contrasts[,i], lfc=log2(minFC))\n  topTags(te, n=nrow(te))    # default sort by pvalue\n})\nnames(li.res.edgeR) &lt;-names(li.GA)[1:3] \n\ndl.res.edgeR&lt;-lapply(li.res.edgeR, function(i)\n  data.table(`gene_id`=rownames(i),i$table)[order(PValue)][,`:=`(\"BH\"=p.adjust(PValue,\"BH\"),\n                                                                  \"BY\"=p.adjust(PValue,\"BY\"),\n                                                                  \"bf\"=p.adjust(PValue,\"bonferroni\"))]\n)\nlapply(dl.res.edgeR, function(i) i[FDR&lt;0.05] %&gt;% nrow)\n\nfwrite(dl.res.edgeR[[\"28wk\"]], file=paste0(\"static/R/result/DEG.edgeR.28wk.\",my.type,\".\",my.salmon.index,\".csv\"))\n\n\n\n\n\n\n2.2.3 The z-score matrix\nThe gene-level count matrix was converted as the unit of CPM (Count Per Million), in log2-scale, via the “cpm” function of edgeR and it was further transformed into a matrix of the z-score using the mean and standard deviation of logCPM from the control samples of each corresponding gestational age group.\n\n\n\n\nListing 2.9: Code to make the z-score matrix from the discovery dataset\n\n\n# CPM based on edgeR TMM  (NB, d isa \"DGEList\")\ndt.logcpm2&lt;-merge(\n  data.table(`geneName`=rownames(d2),cpm(d2,log=T)) %&gt;% \n    melt.data.table(id.vars=c(\"geneName\"),variable.name=\"SampleID\",value.name=\"logCPM\"),\n  dt.samples[Type==my.type,.(SampleID,GA,Condition,Group=paste(GA,Condition,sep=\"-\"))]\n  )\n\ndt.cpmZ=merge(dt.logcpm2,\n              dt.logcpm2[Condition==\"Control\",.(Mean=mean(logCPM),SD=sd(logCPM)),.(GA,geneName)]\n      ,by=c(\"GA\",\"geneName\")\n      )[,.(Group,GA,Condition,SampleID,geneName,logCPM,logCPMZ=(logCPM-Mean)/SD)]\n#save(dt.cpmZ,file=my.cpmZ.RData) # cache to use it later\n\n\n\n\n\n\n2.2.4 Univariate logistic regression\nUsing the binary outcomes, (i.e. case and control status) as dependant variables and the z-scores as independent variables, we applied a generalised linear model for each of the 15,150 genes using the glm function of R stat package (v4.0.3). Both the Akaike information criterion (AIC) and the Bayesian information criterion (BIC) were obtained from the corresponding univariable model and the area under the ROC curve (AUC) was calculated using the pROC package (v1.17.0.1). The p-values were calculated against the null hypothesis that is the odds ratio is equal to 1 and they were adjusted for multiple comparisons using Benjamini and Hochberg method. The distribution of the p-values was tested against a uniform distribution using one-sample Kolmogorov-Smirnov test, which is further explained in Section 6.2.1.\n\n\n\n\nListing 2.10: Code for univariate logisitic regression\n\n\nthis.mat&lt;-dt.cpmZ[,.(SampleID,geneName,logCPMZ,y=ifelse(Condition==\"Case\",1,0))] %&gt;% \n  dcast.data.table(SampleID+y~geneName,value.var=\"logCPMZ\") %&gt;% \n  as.matrix(rownames=\"SampleID\") # logCPMZ\n\nfor(my.GA in names(li.GA)[1:3]){ # for each 12wk, 20wk and 28wkGA\n  this.samples&lt;-colnames(dds.f2)[colData(dds.f2)$GA==my.GA]\n  foo&lt;-list() # per GA\n  for(my.ID in rownames(dds.f2)){\n      #message(paste(my.GA,my.ID))\n      df.mat&lt;-this.mat[this.samples,c(\"y\",my.ID)] %&gt;% as.data.frame\n      my.model&lt;-glm(y ~., data=df.mat, family=\"binomial\")\n      #oddsratio::or_glm(data=df.mat, model=my.model, incr=list(OID20239=1))\n\n      # Log Odds Ratio & p-values & CI\n      #my.model %&gt;% summary\n      #coef(summary(my.model)) # isa 'matrix'\n      foo1&lt;-as.data.frame(coef(summary(my.model)))\n\n      #cbind(coef(my.model), confint(my.model)) # Log Odds Ratio & CI (95%)\n      #exp(cbind(coef(my.model), confint(my.model))) # Odds Ratio & CI (95%)\n      foo2&lt;-as.data.frame(exp(cbind(coef(my.model), confint(my.model)))) # Odds Ratio & CI (95%)\n\n      foo3&lt;-cbind(my.ID,cbind(foo1,foo2)[2,]) %&gt;% data.table \n      colnames(foo3)&lt;-c(\"gene_id\",\"log_odds\",\"se\",\"zval\",\"pval\",\"odds\",\"odds_lo\",\"odds_hi\")\n\n      # ROC & AUC \n      #predict(my.model,type=c(\"response\"))  # predicted probability\n      my.prob&lt;-fitted(my.model) # same as above\n      # no probability in case of NA in the matrix\n      if(nrow(df.mat)!=length(my.prob)){\n          dt.prob&lt;-rbind(\n          data.table(`index`=as.numeric(names(my.prob)),`prob`=my.prob),\n          data.table(`index`=as.numeric(df.mat[,my.ID]%&gt;% is.na %&gt;% which), `prob`=NA)\n          )[order(index)]\n          my.prob&lt;-dt.prob$prob\n      }\n      df.mat$prob&lt;-my.prob\n      my.roc &lt;- pROC::roc(y ~ prob, data = df.mat, quiet=T, ci=TRUE)\n\n      foo[[my.ID]] &lt;- \n      cbind(\n            foo3,\n            data.table(`auc`=my.roc$ci[2], \n                        `auc_lo`=my.roc$ci[1], \n                        `auc_hi`=my.roc$ci[3],\n                        `AIC`=aic(my.model),\n                        `BIC`=bic(my.model) ) \n      )\n  } # end of for   \n  dl.logregZ[[my.GA]]&lt;-rbindlist(foo)\n\n  # apply p.adjust\n  for(i in adjust.methods){\n      dl.logregZ[[my.GA]][,paste0(\"padj.\",i):=p.adjust(pval,i)]\n  }\n} # end of for my.GA\n\n# cache to use it later\n#save(dl.logregZ, file=paste0(\"RData/dl.logregZ.\",my.type,\".\",my.salmon.index,\".RData\"))",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>RNA-seq data processing</span>"
    ]
  },
  {
    "objectID": "quant.html#sec-core-deg",
    "href": "quant.html#sec-core-deg",
    "title": "2  RNA-seq data processing",
    "section": "2.3 Selection of core differentially expressed genes",
    "text": "2.3 Selection of core differentially expressed genes\nTo select a subset of genes that best explains the outcome of samples from the 28wkGA group of the discovery cohort, we used the following four criteria: 1) the p-values from DESeq2, 2) the p-values from edgeR, 3) AIC, and 4) AUC from univariable logistic regressions. Then we selected the top 1% genes for each category (i.e. 151 genes of the lowest p-values from DESeq2 and edgeR, 151 genes having the lowest AIC, and 151 genes having the highest AUC) and constructed a Venn diagram using ggvenn (v0.1.0) R package (52). We selected a total of 17 genes satisfying all the four criteria (i.e. the intersection) and constructed a gene expression matrix (i.e. the 17 genes across the samples from the 28wkGA group of the discovery cohort) using the z-score.\n\n\n\n\nListing 2.11: Code to find the core DEGs\n\n\nvenn.top1pctZ&lt;-list(`DESeq2`=dl.resLFC[[\"28wk\"]][order(pvalue)][1:151]$gene_id,\n                  `edgeR`=dl.res.edgeR[[\"28wk\"]][order(PValue)][1:151]$gene_id,\n                `AUC`=dl.logregZ[[\"28wk\"]][order(-auc)][1:151]$gene_id,\n                `AIC`=dl.logregZ[[\"28wk\"]][order(AIC)][1:151]$gene_id\n                )\n\n# the union of all\ndt.venn.top1pctZ&lt;-lapply(names(venn.top1pctZ), function(i) data.table(i,venn.top1pctZ[[i]])) %&gt;% \n  rbindlist %&gt;% \n  dcast.data.table(V2~i, fun=length)\nsetnames(dt.venn.top1pctZ,\"V2\",\"Gene\")\ntop1pct.genesZ&lt;-dt.venn.top1pctZ[AIC==1 & AUC==1 & DESeq2==1 & edgeR==1]$Gene\ncore17 &lt;- top1pct.genesZ # used in 5-fold CV\n\n\n\n\nThe Venn diagram for the 17 core DEGs is shown in Section 6.2.3.",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>RNA-seq data processing</span>"
    ]
  },
  {
    "objectID": "cross_validation.html",
    "href": "cross_validation.html",
    "title": "3  Cross Validation",
    "section": "",
    "text": "3.1 Data preparation",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "cross_validation.html#sec-cv-prep",
    "href": "cross_validation.html#sec-cv-prep",
    "title": "3  Cross Validation",
    "section": "",
    "text": "3.1.1 Internal validation cohort\nInternal validation dataset was from the Validation dataset shown in Figure 6.1 C.\nNow we set up the Salmon quant files of the validation (i.e. term) dataset. NB, dt.colllDataAll (shown from the code below) was set up from Listing 2.4.\n\n\nCode to set the Salmon quant files of the term dataset (i.e. Validation dataset)\n# remove these two samples as both of them flagged as \"failed\" by illumina (it is a 28wk control sample)\ndt.colDataTerm&lt;-dt.colDataAll[Type==\"term\" & !names %in% c(\"GS-B-374-UW\",\"GS-B-374-UW-b\")] \nli.GA.term&lt;-split(dt.colDataTerm, dt.colDataTerm$GA)\n\n\nWe are ready to read the Salmon quant files via tximeta R package. NB, the code below is equivalent to Listing 2.5 and Listing 2.6 from the discovery cohort (see Section 2.2.1).\n\n\nCode to read the Salmon quant files of the term dataset and make dds object.\nmy.dds.RData&lt;-paste0(\"RData/dds.term.\",my.salmon.index,\".RData\")\nif(file.exists(my.dds.RData)){\n  load(my.dds.RData)\n  message(\"loading dds...\")\n}else{\n  gse.term&lt;-tximeta::tximeta(dt.colDataTerm,skipMeta=T,tx2gene=dt.tx2gene[,.(transcript_id,gene_id)],txOut=F)\n  # set up `dds` at gene-level\n  my.design &lt;- formula(~ Batch + GA + Sex + Condition)      # isa 'formula'\n  dds.term &lt;- DESeqDataSet(se=gse.term, design=my.design) \n\n  dds.term$Group&lt;-factor(paste0(dds.term$GA,dds.term$Condition)) # add 'Group'\n  design(dds.term) &lt;- formula(~ Batch + Sex + Group)        # isa 'formula'\n\n  #\n  dds.term&lt;- DESeq(dds.term, parallel=TRUE) # isa 'DESeqDataSet'\n  save(dds.term,file=my.dds.RData)\n}\n\n# use 15150 genes considered in this study\ndds.f2.term&lt;-DESeq(dds.term[rownames(dds.f2)], parallel=T)\n\nmy.dl.resLFC.RData&lt;-paste0(\"RData/dl.resLFC.term.\",my.salmon.index,\".RData\")\nif(file.exists(my.dl.resLFC.RData)){\n  load(my.dl.resLFC.RData)\n}else{\n  # apply shink\n  li.resLFC.term&lt;-lapply(names(li.GA.term),function(my.GA){\n    my.res&lt;-results(dds.f2.term, \n            #alpha=.05, # by default independant filtering at FDR (alpha) 0.1 (10%) \n            independentFiltering=FALSE,\n            lfcThreshold=log2(minFC), \n            contrast=c(\"Group\",paste0(my.GA,c(\"Case\",\"Control\"))),\n            parallel=TRUE)  \n    lfcShrink(dds.f2.term, \n              #contrast=c(\"Group\",paste0(my.GA,c(\"Case\",\"Control\"))), # not necessary for 'ashr'\n              res=my.res, #li.res[[my.GA]],\n              lfcThreshold=log2(minFC), # not applicable for 'asher' \n              type=\"ashr\",\n              parallel=TRUE)  \n  })\n  names(li.resLFC.term)&lt;-names(li.GA.term)\n\n  dl.resLFC.term&lt;-lapply(li.resLFC.term, function(i)\n    data.table(`gene_id`=rownames(i), as.data.frame(i))[order(pvalue)][,`:=`(\"BH\"=p.adjust(pvalue,\"BH\"),\"BY\"=p.adjust(pvalue,\"BY\"),\"bf\"=p.adjust(pvalue,\"bonferroni\"))]\n  )\n  save(dl.resLFC.term, file=my.dl.resLFC.RData)\n  #fwrite(dl.resLFC[[\"28wk\"]], file=paste0(\"data/DEG.DSeq2.28wk.\",my.type,\".\",my.salmon.index,\".csv\"))\n}\n\n\nNow we use edgeR. NB, this is equivalent to Listing 2.8 shown in Section 2.2.2.\n\n\nCode to find DEGs by edgeR from the term dataset\ngse2 &lt;-gse.term[rownames(dds.f2.term),]\nd2&lt;-tximeta::makeDGEList(gse2)\nd2$samples&lt;-cbind(d2$samples,colData(gse2))\nd2$samples$group&lt;-factor(paste0(d2$samples$GA,d2$samples$Condition)) # add 'group'\nd2$samples$GA&lt;-droplevels(d2$samples$GA) # 36wk removed - really? 2023-03-21\nd2$samples$Batch&lt;-droplevels(d2$samples$Batch) # some batches removed\n\n# TMM normalisation (default). It adds `norm.factors` d2$samples\n# NB, we have `offsets`, which take precedence over lib.size and norm.factors\nd2&lt;-calcNormFactors(d2,method=\"TMM\") \n\n# design\n# my.design &lt;- model.matrix(~ Batch + Sex + group, data=d2$samples)         # isa 'matrix'\n#my.design &lt;- model.matrix(~ 0 + group,  data=d2$samples)       # isa 'matrix'\nmy.design &lt;- model.matrix(~ 0 + group + Sex + Batch , data=d2$samples)      # isa 'matrix'\n(my.contrasts &lt;- makeContrasts(`12wk`=group12wkCase-group12wkControl, \n                              `20wk`=group20wkCase-group20wkControl, \n                              `28wk`=group28wkCase-group28wkControl, \n                              levels=my.design)\n)\n\n# dispersion\n#dp2 = estimateDisp(d2, design=my.design, verbose=T)\ndp2 = estimateGLMCommonDisp(d2, design=my.design, verbose=T) \ndp2 = estimateGLMTrendedDisp(dp2, design=my.design, verbose=T)\ndp2 = estimateGLMTagwiseDisp(dp2, design=my.design)\n\n#f = glmFit(dp2, design=my.design) # \nf = glmQLFit(dp2, design=my.design) # QL(Quasi-like) pipeline\ncolnames(f)\nf$coefficients %&gt;% head\n\n# get the edgeR results\nli.res.edgeR.term&lt;-lapply(names(li.GA.term), function(i){\n  te &lt;- glmTreat(f, contrast=my.contrasts[,i], lfc=log2(minFC))\n  topTags(te, n=nrow(te))    # default sort by pvalue\n})\nnames(li.res.edgeR.term) &lt;-names(li.GA.term)\n\ndl.res.edgeR.term&lt;-lapply(li.res.edgeR.term, function(i)\n  data.table(`gene_id`=rownames(i),i$table)[order(PValue)][,`:=`(\"BH\"=p.adjust(PValue,\"BH\"),\"BY\"=p.adjust(PValue,\"BY\"),\"bf\"=p.adjust(PValue,\"bonferroni\"))]\n)\n\nmy.dl.res.edgeR.RData&lt;-paste0(\"RData/dl.res.edgeR.term.\",my.salmon.index,\".RData\")\nsave(dl.res.edgeR.term, file=my.dl.res.edgeR.RData)\n#fwrite(dl.res.edgeR[[\"28wk\"]], file=paste0(\"data/DEG.edgeR.28wk.\",my.type,\".\",my.salmon.index,\".csv\"))\n\n\nThe gene-level count matrix was converted as the unit of CPM (Count Per Million), in log2-scale, via the “cpm” function of edgeR and it was further transformed into a matrix of the z-score using the mean and standard deviation of logCPM from the control samples of each corresponding gestational age group. NB, this is equivalent to Listing 2.9 shown in Section 2.2.3.\n\n\nCode to make the count matrix from the validation dataset\n# based on genes from dds.f2.term\n# CPM based on edgeR TMM  (NB, d isa \"DGEList\")\nmy.cnt.RData&lt;-paste0(\"RData/dt.count2.term.\",my.salmon.index,\".RData\")\nif(file.exists(my.cnt.RData)){\n  load(my.cnt.RData)\n}else{\n  dt.count&lt;-merge(\n    data.table(`geneName`=rownames(dds.f2.term),counts(dds.f2.term,normalized=T)) %&gt;% \n      melt.data.table(id.vars=c(\"geneName\"),variable.name=\"SampleID\",value.name=\"Count\"),\n    dt.samples[Type==\"term\",.(SampleID,GA,Condition,Group=paste(GA,Condition,sep=\"-\"))]\n  )\n\n  # CPM based on DESeq2 `fpm`\n  dt.cpm&lt;-merge(\n    data.table(`geneName`=rownames(dds.f2.term),fpm(dds.f2.term)) %&gt;% \n      melt.data.table(id.vars=c(\"geneName\"),variable.name=\"SampleID\",value.name=\"CPM\"),\n    dt.samples[Type==\"term\",.(SampleID,GA,Condition,Group=paste(GA,Condition,sep=\"-\"))]\n  )\n\n  dt.tpm&lt;-merge(\n    data.table(`geneName`=rownames(dds.f2.term), assays(dds.f2.term)[[\"abundance\"]]) %&gt;% \n      melt.data.table(id.vars=c(\"geneName\"),variable.name=\"SampleID\",value.name=\"TPM\"),\n    dt.samples[Type==\"term\",.(SampleID,GA,Condition,Group=paste(GA,Condition,sep=\"-\"))]\n  )\n\n  # CPM based on edgeR TMM  (NB, d isa \"DGEList\")\n  dt.cpm2&lt;-merge(\n    data.table(`geneName`=rownames(d2),cpm(d2)) %&gt;% \n      melt.data.table(id.vars=c(\"geneName\"),variable.name=\"SampleID\",value.name=\"CPM\"),\n    dt.samples[Type==\"term\",.(SampleID,GA,Condition,Group=paste(GA,Condition,sep=\"-\"))]\n    )\n\n  dt.logcpm2&lt;-merge(\n    data.table(`geneName`=rownames(d2),cpm(d2,log=T)) %&gt;% \n      melt.data.table(id.vars=c(\"geneName\"),variable.name=\"SampleID\",value.name=\"logCPM\"),\n    dt.samples[Type==\"term\",.(SampleID,GA,Condition,Group=paste(GA,Condition,sep=\"-\"))]\n    )\n\n  save(dt.count, dt.cpm, dt.tpm, dt.cpm2, dt.logcpm2, file=my.cnt.RData)\n}\n\n\n\n\nCode to make the z-score matrix from the validation dataset\nmy.cpmZ.RData&lt;-paste0(\"RData/dt.cpmZ.term.\",my.salmon.index,\".RData\")\nif(file.exists(my.cpmZ.RData)){\n  load(my.cpmZ.RData)\n}else{\n  dt.cpmZ.term=merge(dt.logcpm2,\n                dt.logcpm2[Condition==\"Control\",.(Mean=mean(logCPM),SD=sd(logCPM)),.(GA,geneName)]\n        ,by=c(\"GA\",\"geneName\")\n        )[,.(Group,GA,Condition,SampleID,geneName,logCPM,logCPMZ=(logCPM-Mean)/SD)]\n  save(dt.cpmZ.term,file=my.cpmZ.RData)\n}\n\n\n\n\n3.1.2 External validation dataset\nFor an external validation, we downloaded the raw sequencing counts file (Data File S2: Raw whole-transcriptome sequencing counts for iPEC cohort; n=113) from Munchel et al.. The Data File (in the excel file format) was read and parsed by using readxl (v1.3.1) and data.table (v1.13.6) R packages, respectively.\nFor downstream processing, we only considered those genes in the final set of 15,150 genes that were used in the differentially expressed gene analysis of the discovery and the validation cohort (see Section 2.2.1).\n\n\nCode to import Munchel dataset and make dds object by DESeq2\nmy.dds.RData&lt;-paste0(\"RData/dds.munchel.RData\")\nif(file.exists(my.dds.RData)){\n  load(my.dds.RData)\n  message(\"loading dds...\")\n}else{\n  # import\n  dt.foo&lt;-readxl::read_excel(\"~/data/Munchel/SciTrMed.2020/aaz0131_data_file_s2.xlsx\",skip=3) %&gt;% as.data.table\n  dt.foo[,-c(\"Chr\",\"Start\",\"End\",\"Strand\")][1:5,1:5]\n\n  # sample info with GA of sample collection\n  dt.munchel.meta&lt;-data.table(\n                            names=dt.foo[is.na(Geneid),-c(\"Geneid\",\"Chr\",\"Start\",\"End\",\"Strand\",\"Length\")] %&gt;% \n                              colnames %&gt;% \n                              stringr::str_replace(\"\\\\.\\\\.\\\\.\",\"\"),\n                            GA=dt.foo[is.na(Geneid),-c(\"Geneid\",\"Chr\",\"Start\",\"End\",\"Strand\",\"Length\")] %&gt;% unlist \n                            )\n  dt.munchel.meta[,Condition:=ifelse(grepl(\"PE\",names),\"Case\",\"Control\")]\n  dt.munchel.meta$GA %&gt;% summary\n  dt.munchel.meta[,.N,Condition]\n  dt.munchel.meta[Condition==\"Control\"]$GA %&gt;% summary\n  dt.munchel.meta[Condition==\"Case\"]$GA %&gt;% summary\n  df.munchel.meta&lt;-data.frame(dt.munchel.meta, row.names=dt.munchel.meta$names)\n\n  # cnt \n  dt.munchel.cnt&lt;-dt.foo[!is.na(Geneid),-c(\"Chr\",\"Start\",\"End\",\"Strand\",\"Length\")]\n  dt.munchel.cnt %&gt;% dim # 26708 genes x 114 samples\n  dt.munchel.cnt[1:5,1:5]\n  colnames(dt.munchel.cnt)&lt;-c(\"gene_name\",dt.munchel.meta$names) # update the sample names\n  dt.munchel.cnt[1:5,1:5]\n  dt.munchel.cnt[,.N,gene_name][N&gt;1][order(-N)] # 0 duplicated gene names \n\n  dt.munchel.cnt[grepl(\"_dup\",gene_name)][,.N,gene_name] # 1355 such gene names\n  dt.munchel.cnt[grepl(\"_dup\",gene_name)][1:5,1:5]\n\n  dt.munchel.cnt[,gene_name:=tstrsplit(gene_name,\"_dup\",fixed=T,keep=1L)]\n  dt.munchel.cnt[grepl(\"_dup\",gene_name)]\n  dt.munchel.cnt[,.N,gene_name][N&gt;1][order(-N)] # 491 duplicated gene names\n  dt.munchel.cnt[gene_name==\"REXO1L2P\",1:5]\n  dt.munchel.cnt[,.N,gene_name %in% rownames(dds.f2)] # genes only in the dds.f2 (15150 genes)\n                                                      # TRUE 13555; FALSE 13153\n  rownames(dds.f2) %in% dt.munchel.cnt$gene_name %&gt;% table # from 15150 genesin dds.f2, 13469 genes in Munchel; 1681 genes not in Munchel\n  dds.f2[!rownames(dds.f2) %in% dt.munchel.cnt$gene_name] %&gt;% names\n\n  dt.munchel.cnt2&lt;- (dt.munchel.cnt[gene_name %in% rownames(dds.f2)] %&gt;% \n                     melt.data.table(id.vars=c(\"gene_name\"), variable.name=\"SampleID\",value.name=\"Cnt\"))[,.(Cnt=sum(Cnt)),.(SampleID,gene_name)] %&gt;% \n                      dcast.data.table(gene_name ~ SampleID, value.var=\"Cnt\")\n  dim(dt.munchel.cnt2) # 13469 genes x 114 samples\n  dt.munchel.cnt2[,.N,gene_name][N&gt;1][order(-N)] # no duplicated genes\n  all.equal(colnames(dt.munchel.cnt),colnames(dt.munchel.cnt2))\n\n  mat.munchel.cnt2&lt;-dt.munchel.cnt2 %&gt;% as.matrix(rownames=\"gene_name\")\n  dim(mat.munchel.cnt2) # 13469 x 113\n  mat.munchel.cnt2[1:5,1:5]\n\n  all.equal(colnames(mat.munchel.cnt2), rownames(df.munchel.meta))\n\n  dds.munchel&lt;-DESeqDataSetFromMatrix(mat.munchel.cnt2, df.munchel.meta, design=formula(~Condition) )\n\n  dds.munchel&lt;- DESeq(dds.munchel, parallel=TRUE) # isa 'DESeqDataSet'\n  save(dt.munchel.meta,dds.munchel,file=my.dds.RData)\n}\n\n\nUsing the filtered gene-level raw count matrix, we ran edgeR and constructed a matrix of CPM, in log2-scale, via the “cpm” function of edgeR. The matrix was further transformed into a matrix of the z-score using the mean and standard deviation of logCPM from the 73 control samples.\n\n\nCode to make the z-score matrix from the Munchel dataset\nmy.cpmZ.RData&lt;-paste0(\"RData/dt.cpmZ.munchel.RData\")\nif(file.exists(my.cpmZ.RData)){\n  load(my.cpmZ.RData)\n}else{\n  load(\"RData/dds.munchel.RData\")\n  dds.munchel # samples from Discovery & Validation1\n\n  d.munchel = DEFormats::as.DGEList(dds.munchel)\n  d.munchel&lt;-calcNormFactors(d.munchel,method=\"TMM\") \n\n  dt.logcpm2&lt;-merge(\n      data.table(`geneName`=rownames(d.munchel),cpm(d.munchel,log=T)) %&gt;% \n        melt.data.table(id.vars=c(\"geneName\"),variable.name=\"SampleID\",value.name=\"logCPM\"),\n      df.munchel.meta,by.x=\"SampleID\",by.y=\"names\"\n      )\n\n  dt.cpmZ.munchel=merge(dt.logcpm2,\n                dt.logcpm2[Condition==\"Control\",.(Mean=mean(logCPM),SD=sd(logCPM)),.(geneName)]\n        ,by=c(\"geneName\")\n        )[,.(Condition,SampleID,geneName,logCPM,logCPMZ=(logCPM-Mean)/SD)]\n  save(dt.cpmZ.munchel,file=my.cpmZ.RData)\n}",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "cross_validation.html#sec-cv-split",
    "href": "cross_validation.html#sec-cv-split",
    "title": "3  Cross Validation",
    "section": "3.2 Data split for 5-fold CV",
    "text": "3.2 Data split for 5-fold CV\nWe randomly split the samples into 5 strata by distributing the number of case and control outcomes as even as possible across the 5 folds. This stratified 5-fold splitting was repeated 5 times by changing a seed number in each repetition, and the 11 ML models (see below #sec-cv-11ML) were trained to choose a desired number of predictors from 2 to 6.\n\n\nCode to split 5-fold with 5 repetitions\nload(\"RData/dt.cpmZ.preterm.POPS-2022.GRCh38.88.RData\") # dt.cpmZ (preterm)\nload(\"RData/dt.cpmZ.term.POPS-2022.GRCh38.88.RData\") # dt.cpmZ.term (term)\nload(\"RData/dt.cpmZ.munchel.RData\") # dt.cpmZ.munchel (Munchel)\n\nli.mat&lt;-list()\n# set the train dataset, i.e. preterm-28wk\nli.mat[[\"train\"]]&lt;-lapply(list(`12wk`=\"12wk\",`20wk`=\"20wk\",`28wk`=\"28wk\"), function(my.GA) {\n  dt.cpmZ[GA==my.GA & geneName %in% core17,.(SampleID,geneName,logCPMZ,y=ifelse(Condition==\"Case\",1,0))] %&gt;% dcast.data.table(SampleID+y~geneName,value.var=\"logCPMZ\") %&gt;% as.matrix(rownames=\"SampleID\") # isa 'list'\n  })\n\n# set the test dataset, i.e. term\nli.mat[[\"test\"]]&lt;-lapply(list(`12wk`=\"12wk\",`20wk`=\"20wk\",`28wk`=\"28wk\",`36wk`=\"36wk\"), function(my.GA) {\n    dt.cpmZ.term[GA==my.GA & geneName %in% core17,.(SampleID,geneName,logCPMZ,y=ifelse(Condition==\"Case\",1,0))] %&gt;% dcast.data.table(SampleID+y~geneName,value.var=\"logCPMZ\") %&gt;% as.matrix(rownames=\"SampleID\") # isa 'list'\n  })\n\nli.mat[[\"munchel\"]]&lt;-dt.cpmZ.munchel[geneName %in% core17,.(SampleID,geneName,logCPMZ,y=ifelse(Condition==\"Case\",1,0))] %&gt;% dcast.data.table(SampleID+y~geneName,value.var=\"logCPMZ\") %&gt;% as.matrix(rownames=\"SampleID\") # isa 'matrix'\n\n##############################################\n# Set the 5-fold with 5 rep for 28wk preterm #\n##############################################\n# set stratified folds\n# such that the numbers of cases and controls in each fold are the same for each fold (or, at least, as close to this as possible)\nnFold&lt;-5; nRep&lt;-5; li.fold&lt;-list() # index of training in \nfor(iRep in 1:nRep){\n  caseInds &lt;- which(li.mat[[\"train\"]][[\"28wk\"]][,\"y\"]==1)\n  ctrlInds &lt;- which(li.mat[[\"train\"]][[\"28wk\"]][,\"y\"]==0)\n\n  #Randomise for good measure:\n  set.seed(123+iRep)\n  caseInds &lt;- caseInds[sample(1:length(caseInds))]\n  ctrlInds &lt;- ctrlInds[sample(1:length(ctrlInds))]\n\n  approximatelyEqualParts_Cases &lt;- ggplot2::cut_interval(1:length(caseInds), nFold)\n  approximatelyEqualParts_Ctrls &lt;- ggplot2::cut_interval(1:length(ctrlInds), nFold)\n\n  quintiles_Cases &lt;- vector(mode = \"integer\", length = length(caseInds))\n  quintiles_Ctrls &lt;- vector(mode = \"integer\", length = length(ctrlInds))\n  for(i in 1:length(levels(approximatelyEqualParts_Cases))){\n    currentLevel &lt;- levels(approximatelyEqualParts_Cases)[i]\n    quintiles_Cases[approximatelyEqualParts_Cases == currentLevel] &lt;- i\n  }\n\n  for(i in 1:length(levels(approximatelyEqualParts_Ctrls))){\n    currentLevel &lt;- levels(approximatelyEqualParts_Ctrls)[i]\n    quintiles_Ctrls[approximatelyEqualParts_Ctrls == currentLevel] &lt;- i\n  }\n\n  quintiles &lt;- vector(mode = \"integer\", length = nrow(li.mat[[\"train\"]][[\"28wk\"]]))\n  for(i in 1:nFold){\n    quintiles[c(caseInds[quintiles_Cases == i], ctrlInds[quintiles_Ctrls == i])] &lt;- i\n  }\n\n    # Split the data into training and testing sets for this fold\n  for(iFold in 1:nFold){\n    li.fold[[paste0(\"Fold\",iFold,\".Rep\",iRep)]]&lt;-which(quintiles != iFold)\n  }\n} # end of iRep\n\n########################################\n# Set the final 5-fold to use all_28wk #\n########################################\nnFold&lt;-5; nRep&lt;-1; li.fold.final&lt;-list() # index of training in \nfor(iRep in 1:nRep){\n  caseInds &lt;- which(li.mat[[\"train\"]][[\"28wk\"]][,\"y\"]==1)\n  ctrlInds &lt;- which(li.mat[[\"train\"]][[\"28wk\"]][,\"y\"]==0)\n\n  #Randomise for good measure:\n  set.seed(333)\n  caseInds &lt;- caseInds[sample(1:length(caseInds))]\n  ctrlInds &lt;- ctrlInds[sample(1:length(ctrlInds))]\n\n  approximatelyEqualParts_Cases &lt;- ggplot2::cut_interval(1:length(caseInds), nFold)\n  approximatelyEqualParts_Ctrls &lt;- ggplot2::cut_interval(1:length(ctrlInds), nFold)\n\n  quintiles_Cases &lt;- vector(mode = \"integer\", length = length(caseInds))\n  quintiles_Ctrls &lt;- vector(mode = \"integer\", length = length(ctrlInds))\n  for(i in 1:length(levels(approximatelyEqualParts_Cases))){\n    currentLevel &lt;- levels(approximatelyEqualParts_Cases)[i]\n    quintiles_Cases[approximatelyEqualParts_Cases == currentLevel] &lt;- i\n  }\n\n  for(i in 1:length(levels(approximatelyEqualParts_Ctrls))){\n    currentLevel &lt;- levels(approximatelyEqualParts_Ctrls)[i]\n    quintiles_Ctrls[approximatelyEqualParts_Ctrls == currentLevel] &lt;- i\n  }\n\n  quintiles &lt;- vector(mode = \"integer\", length = nrow(li.mat[[\"train\"]][[\"28wk\"]]))\n  for(i in 1:nFold){\n    quintiles[c(caseInds[quintiles_Cases == i], ctrlInds[quintiles_Ctrls == i])] &lt;- i\n  }\n\n    # Split the data into training and testing sets for this fold\n  for(iFold in 1:nFold){\n    li.fold.final[[paste0(\"Fold\",iFold,\".Rep\",iRep)]]&lt;-which(quintiles != iFold)\n  }\n} # end of iRep",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "cross_validation.html#sec-cv-11ML",
    "href": "cross_validation.html#sec-cv-11ML",
    "title": "3  Cross Validation",
    "section": "3.3 11 machine learning methods",
    "text": "3.3 11 machine learning methods\nWe considered a total of 11 ML methods to select the best performing method based on the 5-fold cross-validation (CV) with 5 repetitions.\n\n\n\n\n\n\nFigure 3.1: 11 machine-learning methods considered in this study\n\n\n\nFor the three penalised regression methods (ENet1, ENet2 and LASSO), they were firstly fitted by using the train function for the two Elastic net methods (ENet1 and ENet2) and the cv.glmnet function for LASSO, from the caret (v6.0.94) and the glmnet (v.4.1.2) R package, respectively. For ENet1, both the parameter \\(\\alpha\\) and \\(\\lambda\\) were tuned by the caret::train, whereas the parameter \\(\\lambda\\) was further tuned by the glmnet::cv.glmnet for ENet2.\nNext, based on the best fitted penalised regression models, a matrix of the \\(\\beta\\) coefficient was examined to find the first set of predictors with non-zero \\(\\beta\\) coefficients that satisfied a desired number of predictors. If the number of predictors with non-zero coefficients exceeded the desired number, the absolute values of coefficients were sorted in their decreasing order and only the desired number of predictors were selected with their highest absolute scores.\nFor the remaining methods, except mSVM-RFE (see also Section 3.3.2) which embedded a Recursive Feature Elimination (RFE) algorithm internally, we used the caret::rfe function by controlling the “sizes” parameter to have the corresponding models with the desired number of predictors.\n\n3.3.1 glParallel\nIn glParallel, a brute-force exhaustive search method, for a given number of predictors, it searched all possible combinations of predictors in multivariate regression models and picked the best model based on the highest predictive performance.\nFor example, glParallel trained a total of 2,380 models, which is the possible number of combinations having 4 predictors out of 17, and chose the best model based on the highest Leave Pair Out Cross Validated (LPOCV) Area Under the ROC Curve (AUC), a version of optimism-corrected AUC.\n\n\n\n\n\n\nInstall glParallel\n\n\n\nYou need to download and install glParallel locally via the following git command:\n\n\n\nListing 3.1: Code to download glParallel\n\n\ngit clone https://gitlab.developers.cam.ac.uk/ssg29/glparallel.git static/R/glParallel\n\n\n\n\n\nThen, prepare dataset to run glParallel.\n\n\nCode to prepare dataset for glParallel\n#####################################################\n# make training dataset files for the CV glParallel #\n#####################################################\nlapply(names(li.fold), function(i){\n  message(\"fold=\",i)\n  # training set\n  my.index=li.fold[[i]]\n  my.file.name=file.path(\"glParallel/data\",paste(\"core17\",i,\"csv\",sep=\".\"))\n  fwrite(li.mat[[\"train\"]][[\"28wk\"]][my.index,], file=my.file.name)\n})\n# then run glParallel (see glParallel/RUN)\n\n####################################################\n# make training dataset files for final glParallel #\n####################################################\nfwrite(li.mat[[\"train\"]][[\"28wk\"]], file=\"glParallel/data/core17.final.csv\")\n\n\n\n\n\n\n\n\nLPOCV-AUC\n\n\n\nIn LPOCV-AUC, a model was fitted based on a given set of training samples except one pair of case-and-control, then the model was used to predict the outcome of the remaining pair. The LPOCV-AUC was calculated as the proportion of all pairwise combinations in which the predicted probability was greater for the case than for the control. There is a helper function to calculate the LPOCV-AUC in glParallel\n\n\n\n\n3.3.2 mSVM-RFE\nThis method is from the (multiple) Support Vector Machine Recursive Feature Elimination (mSVM-RFE).\n\n\n\n\n\n\nInstall SVM-RFE\n\n\n\nYou need to download nad install SVM-RFE locally via the following git command:\n\n\n\nListing 3.2: Code to download SVM-RFE\n\n\ngit clone https://github.com/johncolby/SVM-RFE.git static/R/SVM-RFE",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "cross_validation.html#sec-5fold-cv",
    "href": "cross_validation.html#sec-5fold-cv",
    "title": "3  Cross Validation",
    "section": "3.4 5-Fold cross validation",
    "text": "3.4 5-Fold cross validation\nWe defined a series of helper functions to facilitate the whole process of CV more efficient.\n\n\n\n\n\n\nR function get_lasso_coef\n\n\n\nThis helper function runs LASSO and ranks features by their importance.\n\n\nCode to run Lasso and get non-zero coefficient\n# returns: data.table(`method`,`fold`,`feature`,`score`,`rank`)\nget_lasso_coef&lt;-function(x,my.fold,my.num=4){\n  #x isa `matrix` and should contain 'y' column\n  all.features&lt;-colnames(x)[colnames(x)!=\"y\"]\n\n  #############\n  # run Lasso #\n  #############\n  set.seed(333) # set a random seed for a reproducibility\n  system.time(\n      cv.fit&lt;-cv.glmnet(\n                  x= x[,all.features], \n                  y= x[,'y'], \n                  family=\"binomial\",\n                  alpha=1, # default (i.e. lasso)\n                  keep=T, # FALSE by default\n                  type.measure = \"auc\" #type.measure=\"class\" # default for 'binomial'\n      )\n  )\n\n  ############################################\n  # 1. select by lambda.min  & 2. lambda.1se #\n  ############################################\n  dt.foo&lt;-lapply(c(\"lambda.min\",\"lambda.1se\"), function(my.lambda){\n    coeff1&lt;-coef(cv.fit, s = my.lambda) %&gt;% as.matrix #Extract coefficients from this glmnet object\n    nZero1&lt;-coeff1[coeff1[,\"s1\"]!=0,,drop=F][-1,,drop=F] %&gt;% nrow # the number of non-zero coeff\n    if(nZero1==0){\n      dt.foo&lt;-data.table(\n                        method=my.lambda,\n                        fold=my.fold,\n                        `feature`=NA,\n                        score=NA)\n    }else{\n      coeff1[coeff1[,\"s1\"]!=0,,drop=F][-1,,drop=F] \n      coeff1[coeff1[,\"s1\"]!=0,,drop=F][-1,,drop=F] %&gt;% as.data.table(keep.rownames=T) # return DT(rn,s1)\n      dt.foo&lt;-data.table(\n                        method=my.lambda,\n                        fold=my.fold,\n                        coeff1[coeff1[,\"s1\"]!=0,,drop=F][-1,,drop=F] %&gt;% as.data.table(keep.rownames=T))\n    }\n    setnames(dt.foo,c(\"method\",\"fold\",\"feature\",\"score\"))\n    dt.foo\n  }) %&gt;% rbindlist\n\n  ####################\n  # 3. Lasso-pathway #\n  ####################\n  mat.beta &lt;- cv.fit$glmnet.fit$beta %&gt;% as.matrix\n  apply(mat.beta, 2, function(i){table(i!=0)[\"TRUE\"]})\n  my.lambdas&lt;-apply(mat.beta, 2, function(i){table(i!=0)[\"TRUE\"]})&gt;=my.num\n  this.index&lt;-my.lambdas[my.lambdas & !is.na(my.lambdas)][1] %&gt;% names # the first index &gt;=my.num\n  if(is.na(this.index)){\n    NULL\n  }else{\n    this.index.num &lt;- (strsplit(this.index,\"s\")[[1]][2] %&gt;% as.integer) +1\n    nZero&lt;-sum(mat.beta[,this.index]!=0, na.rm=T) # number of non-zero coefficient\n    #mat.beta[mat.beta[,this.index]!=0,this.index,drop=F]\n    dt.bar&lt;-data.table(\n                      method=\"LASSO\",\n                      fold=my.fold,\n                      mat.beta[mat.beta[,this.index.num]!=0,this.index.num,drop=F] %&gt;% as.data.table(keep.rownames=T)\n    ) # save as the above\n    setnames(dt.bar,c(\"method\",\"fold\",\"feature\",\"score\"))\n\n    if(nrow(dt.bar)&gt;my.num){\n      dt.bar&lt;-dt.bar[order(method,fold,-abs(score))][1:my.num]\n    }\n\n    dt.baz&lt;-rbind(dt.foo, dt.bar)\n    dt.baz&lt;-dt.baz[order(method,fold,-abs(score))][,rank:=1:.N,.(method,fold)]\n    return(dt.baz)\n  }\n} # end of get_lasso_coef\n\n\n\n\n\n\n\n\n\n\nR function get_enet_coef\n\n\n\nThis helper function runs Elastic net and rank features by their importance.\n\n\nCode to run ENet1 and ENet2 and get non-zero coefficients\n# returns: data.table(`method`,`fold`,`feature`,`score`,`rank`)\nget_enet_coef&lt;-function(x,my.fold,my.num=4){\n  #x isa `matrix` and should contain 'y' column\n  all.features&lt;-colnames(x)[colnames(x)!=\"y\"]\n\n  cv.fit&lt;-list()\n  ##########\n  # E: EN1 #\n  ##########\n  cl &lt;- makePSOCKcluster(8) # No. of cores to use\n  registerDoParallel(cl)\n  set.seed(333) # set a random seed for a reproducibility\n  system.time(\n      cv.fit[[\"E\"]]&lt;-caret::train(\n                              x= x[,all.features], \n                              y=factor(ifelse(x[,\"y\"]==1,'case','non_case'),levels=c(\"non_case\",\"case\")),\n                              method=\"glmnet\",\n                              family=\"binomial\",\n                              trControl = trainControl(method = \"cv\",\n                                                        summaryFunction = twoClassSummary,\n                                                        classProbs = TRUE,\n                                                        savePredictions = T,\n                                                        verboseIter = T,\n                                                        ),  # number =10 by default for \"cv\"\n                              tuneLength=10, # grid size: 10(alpha) * 10(lambda) \n      )\n  )\n  stopCluster(cl)\n\n  if(F){\n  varImp(cv.fit[[\"E\"]], useModel=T)\n  varImp(cv.fit[[\"E\"]], useModel=F, nonpara=F, scale=T)\n  predictors(cv.fit$E) # features used in the model\n  }\n\n  mat.beta&lt;- cv.fit$E$finalModel$beta %&gt;% as.matrix\n  my.lambdas&lt;-apply(mat.beta, 2, function(i){table(i!=0)[\"TRUE\"]})&gt;=my.num\n  this.index&lt;-my.lambdas[my.lambdas & !is.na(my.lambdas)][1] %&gt;% names\n  if(is.na(this.index)){\n    dt.foo&lt;-NULL\n  }else{\n    this.index.num &lt;- (strsplit(this.index,\"s\")[[1]][2] %&gt;% as.integer) +1\n    #mat.beta[mat.beta[,this.index.num]!=0,this.index.num,drop=F] %&gt;% as.data.table(keep.rownames=T)\n    dt.foo&lt;-data.table(method=\"ENet1\",\n                      fold=my.fold,\n                      mat.beta[mat.beta[,this.index.num]!=0,this.index.num,drop=F] %&gt;% as.data.table(keep.rownames=T)) # save as the above\n    setnames(dt.foo,c(\"method\",\"fold\",\"feature\",\"score\"))\n    if(nrow(dt.foo)&gt;my.num){\n      dt.foo&lt;-dt.foo[order(method,fold,-abs(score))][1:my.num]\n    }\n  }\n\n  ##########\n  # F: EN2 #\n  ##########\n  set.seed(333)\n  system.time(\n      cv.fit[[\"F\"]]&lt;-cv.glmnet(\n                  x= x[,all.features], \n                  y= x[,\"y\"], # will be coerced to a factor if not (for binomial)\n                  family=\"binomial\",\n                  alpha=cv.fit$E$bestTune$alpha,\n                  keep=T, # FALSE by default\n                  type.measure = \"auc\" #type.measure=\"class\" # default for 'binomial'\n      )\n  )\n  #cv.fit$F$nzero\n  mat.beta2 &lt;- cv.fit$F$glmnet.fit$beta %&gt;% as.matrix\n  my.lambdas&lt;-apply(mat.beta2, 2, function(i){table(i!=0)[\"TRUE\"]})&gt;=my.num\n  this.index&lt;-my.lambdas[my.lambdas & !is.na(my.lambdas)][1] %&gt;% names\n  if(is.na(this.index)){\n    dt.bar&lt;-NULL\n  }else{\n    this.index.num &lt;- (strsplit(this.index,\"s\")[[1]][2] %&gt;% as.integer) +1\n    dt.bar&lt;-data.table(\n                        method=\"ENet2\",\n                        fold=my.fold,\n                        mat.beta2[mat.beta2[,this.index.num]!=0,this.index.num,drop=F] %&gt;% as.data.table(keep.rownames=T)) # save as the above\n    setnames(dt.bar,c(\"method\",\"fold\",\"feature\",\"score\"))\n    if(nrow(dt.bar)&gt;my.num){\n      dt.bar&lt;-dt.bar[order(method,fold,-abs(score))][1:my.num]\n    }\n  }\n\n  dt.baz&lt;-rbind(dt.foo,dt.bar)\n  if(!is.null(dt.baz)){\n    dt.baz&lt;-dt.baz[order(method,fold,-abs(score))][,rank:=1:.N,.(method,fold)]\n  }\n  return(dt.baz)\n} # end of get_enet_coef\n\n\n\n\n\n\n\n\n\n\nR function runRFE2\n\n\n\nThis helper function select desired number of features by using recursive feature elimination method via caret::rfe()\n\n\nCode to run recursive feature elimination\n# x: data matrix\n# my.method: \n# my.num: number of desired features\n# my.index: a list with elements for each external resampling iteration.\n# is.final: the final selected features if set true; oterwise at each fold level\n# returns: data.table(`method`,`fold`,`feature`,`score`,`rank`)\nrunRFE2 &lt;-function(x, my.method=\"svmRadial\", my.num=4, my.index, is.final=F){\n  #x isa `matrix` and should contain 'y' column\n  all.features&lt;-colnames(x)[colnames(x)!=\"y\"]\n\n  li.methods&lt;-list(\n    `svmLinear`=\"svmLinear\",\n    `svmRadial`=\"svmRadial\",\n    `nnet`=\"nnet\",\n    `pcaNNet`=\"pcaNNet\",\n    `rfFuncs`=rfFuncs,\n    `nbFuncs`=nbFuncs,\n    )\n\n  my.fun&lt;-li.methods[[my.method]]\n  if(is.list(my.fun)){\n    myFuncs&lt;-my.fun\n  }else{\n    myFuncs&lt;-caretFuncs\n  }\n  myFuncs$summary &lt;- twoClassSummary\n\n  rfe.ctrl &lt;- rfeControl(functions=myFuncs,\n                         method = \"cv\",\n                         #repeats =1, number = 10, # NB, index below\n                         #returnResamp=\"all\", # \"final\" by default\n                         saveDetails=T,\n                         verbose = TRUE,\n                         index = my.index #a list with elements for each external resampling iteration.\n                                          #Each list element is the sample rows used for training at\n                                          #that iteration.\n  )\n\n  tr.ctrl &lt;- trainControl(method = \"cv\",\n                          #repeats =1, number = 10, # NB, index below\n                          summaryFunction = twoClassSummary,\n                          classProbs = TRUE,\n                          savePredictions = T,\n                          verboseIter = T,\n                          index = my.index #a list with elements for each external resampling iteration.\n                                            #Each list element is the sample rows used for training at\n                                            #that iteration.\n  )\n\n  # run REF via caret::ref #\n  cl &lt;- makePSOCKcluster(8) # No. of cores to use\n  registerDoParallel(cl)\n  set.seed(333) # set a random seed for a reproducibility\n  cv.rfe&lt;-caret::rfe(\n                     x=as.data.frame(x[,all.features]),\n                     y=factor(ifelse(x[,\"y\"]==1,'case','non_case'),levels=c(\"non_case\",\"case\")),\n                     sizes = my.num, #2^(2:4), # default \n                     metric=\"ROC\",\n                     rfeControl=rfe.ctrl,\n                     method=my.method, # feed it to 'train'\n                     tuneLength = 5, #ifelse(trControl$method == \"none\", 1, 3)\n                     trControl=tr.ctrl # feed it to 'train'\n  )\n  stopCluster(cl)\n\n  dt.foo&lt;-data.table(`method`=my.method,cv.rfe$variables)\n  if(is.final){\n    dt.bar&lt;-data.table(`method`=my.method,\n                       `fold`=\"final\",\n                       `feature`=cv.rfe$optVariables[1:my.num],\n                       `score`=NA,\n                       `rank`=1:my.num)\n  }else{\n    dt.bar&lt;-dt.foo[Variables==cv.rfe$optsize][order(Resample,-Overall)][,.SD[1:my.num],.(method,Resample)][,rank:=1:.N,.(method,Resample)][,.(method,fold=Resample,feature=var,score=Overall,rank)]\n  }\n\n  return(dt.bar)\n}\n\n\n\n\n\n\n\n\n\n\nR function get_cv_glm\n\n\n\nThis helper function extracts the predictive performance of the training model from a given fold tested using held-out samples during kCV.\n\n\nCode to extract the stat of the training model given the fold ID from the 5-fold CV with 5 repetitions\n# x: matrix dataset (train/test at the same time, separated by my.index which is the training)\n# my.fold: fold ID\n# my.index: the training index of `x` to subset where the model should be built\n# my.feature: features of interests\nget_cv_glm&lt;-function(x=li.mat[[\"train\"]][[\"28wk\"]],my.fold,my.index,my.feature){\n  mat.tr&lt;-x[my.index,] # index of the training\n  df.mat.tr&lt;-mat.tr[,c(my.feature,'y')] %&gt;% as.data.frame  # training set\n\n  ############################################\n  # fit the model using the training dataset #\n  ############################################\n  my.model&lt;-glm(y~. , data = df.mat.tr, family = \"binomial\")\n\n  # ROC from the training fold\n  my.roc &lt;-pROC::roc(response=df.mat.tr$y, predictor=fitted(my.model),quite=T,ci=T)\n\n  # LPOCV from the training fold\n  LPOCV.boot&lt;-boot::boot(data=df.mat.tr, statistic=get_LPOCV_boot,R=100,parallel=\"multicore\",ncpus=10)\n  LPOCV.ci&lt;-boot::boot.ci(LPOCV.boot,type=\"perc\")\n\n  # the whole dataset were training set, i.e. no test\n  if(length(my.index)==nrow(x)){\n    cbind(\n      data.table(\n                `fold`=my.fold,\n                `predictor`=paste(my.feature,collapse=\",\")\n                ),\n      data.table(\n                `AIC`=my.model$aic,\n                `BIC`=BIC(my.model),\n                `AUC`=my.roc$ci[2]*100, \n                `AUC_lo`=my.roc$ci[1]*100, \n                `AUC_hi`=my.roc$ci[3]*100,\n                `LPOCV`=LPOCV.boot$t0,\n                `LPOCV_lo`=LPOCV.ci$percent[4],\n                `LPOCV_hi`=LPOCV.ci$percent[5]\n                )\n    )\n  }else{\n    #################################################\n    # now, predict the outcome of the held-out data #\n    # using the model from the training dataset     #\n    #################################################\n    mat.test&lt;-x[-my.index,]  \n    df.mat.test&lt;-mat.test[,c(my.feature,'y')] %&gt;% as.data.frame # held-out\n\n    my.prob&lt;-predict.glm(my.model, newdata=df.mat.test, type=\"response\") \n    my.roc.test &lt;- pROC::roc(response=df.mat.test$y, predictor=my.prob,quiet=T,ci=T)\n    LPOCV.boot.test&lt;-boot::boot(data=df.mat.test, statistic=get_LPOCV_boot,R=100,parallel=\"multicore\",ncpus=20)\n    # deal with failed boot result\n    if(is.numeric(LPOCV.boot.test$t)){\n      LPOCV.ci.test&lt;-boot::boot.ci(LPOCV.boot.test,type=\"perc\")\n      LPOCV_test_lo=LPOCV.ci.test$percent[4]\n      LPOCV_test_hi=LPOCV.ci.test$percent[5]\n    }else{\n      LPOCV_test_lo=NA\n      LPOCV_test_hi=NA\n    }\n    # return the following table\n    cbind(\n      data.table(\n                `fold`=my.fold,\n                `predictor`=paste(my.feature,collapse=\",\")\n                ),\n      data.table(\n                `AIC`=my.model$aic,\n                `BIC`=BIC(my.model),\n                `AUC`=my.roc$ci[2]*100, \n                `AUC_lo`=my.roc$ci[1]*100, \n                `AUC_hi`=my.roc$ci[3]*100,\n                `LPOCV`=LPOCV.boot$t0,\n                `LPOCV_lo`=LPOCV.ci$percent[4],\n                `LPOCV_hi`=LPOCV.ci$percent[5],\n                `AUC_test`=my.roc.test$ci[2]*100, \n                `AUC_test_lo`=my.roc.test$ci[1]*100, \n                `AUC_test_hi`=my.roc.test$ci[3]*100,\n                `LPOCV_test`=LPOCV.boot.test$t0,\n                `LPOCV_test_lo`=LPOCV_test_lo,\n                `LPOCV_test_hi`=LPOCV_test_hi\n                )\n    )\n  }\n}\n\n\n\n\n\n\n\n\n\n\nR function get_cv_glm2\n\n\n\nThis helper function extracts the basic statistic (e.g. AIC, BIC, and AUC etc) of the training model for a given fold during kCV.\n\n\nCode to extract the predictive performance of the training model given the fold ID from the 5-fold CV with 5 repetitions\n# x: the dataset where the model should be tested on (i.e. the test set)\n# my.model: the model from the training \n# my.feature: features of interests\nget_cv_glm2&lt;-function(x=li.mat[[\"test\"]][[\"28wk\"]],my.fold, my.model,my.feature){\n    df.mat.test&lt;-x[,c(my.feature,'y')] %&gt;% as.data.frame # test (validation) dataset\n\n    #my.prob&lt;-predict.glm(my.model, newdata=df.mat.test, type=\"response\") \n    my.prob&lt;-predict(my.model, newdata=df.mat.test, type=\"response\") \n    my.roc.test &lt;- pROC::roc(response=df.mat.test$y, predictor=my.prob,quiet=T,ci=T)\n    LPOCV.boot.test&lt;-boot::boot(data=df.mat.test, statistic=get_LPOCV_boot,R=100,parallel=\"multicore\",ncpus=20)\n    # deal with failed boot result\n    if(is.numeric(LPOCV.boot.test$t)){\n      LPOCV.ci.test&lt;-boot::boot.ci(LPOCV.boot.test,type=\"perc\")\n      LPOCV_test_lo=LPOCV.ci.test$percent[4]\n      LPOCV_test_hi=LPOCV.ci.test$percent[5]\n    }else{\n      LPOCV_test_lo=NA\n      LPOCV_test_hi=NA\n    }\n\n    # return the following table\n    cbind(\n      data.table(\n                `fold`=my.fold,\n                `predictor`=paste(my.feature,collapse=\",\")\n                ),\n      data.table(\n                `AUC_test`=my.roc.test$ci[2]*100, \n                `AUC_test_lo`=my.roc.test$ci[1]*100, \n                `AUC_test_hi`=my.roc.test$ci[3]*100,\n                `LPOCV_test`=LPOCV.boot.test$t0,\n                `LPOCV_test_lo`=LPOCV_test_lo,\n                `LPOCV_test_hi`=LPOCV_test_hi,\n                 `AIC`=my.model$aic,\n                 `BIC`=BIC(my.model)\n                )\n    )\n}\n\n\n\n\nHaving defined those helper functions, we are now ready to proceed 5-fold CV with 5 repetitions.\n\n\nCode to run 11 ML methods in 5-fold CV with 5-repetitions\n## \n## Training on based on 5-fold CV\n##\nmy.RData&lt;-file.path(\"RData/dl.kcv.core17.RData\")\nif(file.exists(my.RData)){\n  load(my.RData)\n}else{\n  library(e1071)\n  source('SVM-RFE/msvmRFE.R')\n\n  li.methods&lt;-list(\n    `svmLinear`=\"svmLinear\",\n    `svmRadial`=\"svmRadial\",\n    `nnet`=\"nnet\",\n    `pcaNNet`=\"pcaNNet\",\n    `rfFuncs`=rfFuncs,\n    `nbFuncs`=nbFuncs\n    )\n\n  #|Num: 6 Method: LASSO\n  # Fold5.Rep5\n  #Error in mat.beta[, this.index] : subscript out of bounds\n  li.num&lt;-list(`F2`=2,`F3`=3,`F4`=4,`F5`=5,`F6`=6)\n  dl.kcv&lt;-lapply(li.num, function(my.num){\n    # 1. glParallel\n    message(paste(\"Num:\",my.num,\"Method: glParallel\"))\n    my.pattern=paste0(\"^core17\\\\.Fold[[:alnum:]]\\\\.Rep[[:alnum:]]\\\\.best\",my.num)\n    dt.input&lt;-data.table(\n                    foo=list.files(\"glParallel/result\",pattern=my.pattern),\n                    files=list.files(\"glParallel/result\",pattern=my.pattern,full.names=T)\n                    )[,c(\"foo1\",\"bar1\"):=tstrsplit(foo,\"\\\\.\",keep=c(2,3))][,fold:=paste(foo1,bar1,sep=\".\")][,c(\"foo\",\"foo1\",\"bar1\"):=NULL]\n\n    dt.cv.glp &lt;-apply(dt.input, 1, function(i){\n              my.fold&lt;-i[[\"fold\"]]\n              my.feature&lt;-fread(i[\"files\"])[1][[\"Best proteins\"]] %&gt;% strsplit(\",\") %&gt;% unlist\n              data.table(method=\"glParallel\",fold=my.fold,feature=my.feature,score=NA,rank=NA)\n              #cbind(`fold`=i[[\"fold\"]],fread(i[\"files\"])[1])\n    }) %&gt;% rbindlist\n\n    # 2. Lasso\n    # one multinomial or binomial class has fewer than 8  observations; dangerous ground\n    dt.cv.lasso&lt;-lapply(names(li.fold), function(my.fold){\n      message(paste(\"Num:\",my.num,\"Method: LASSO, Fold:\",my.fold))\n      my.tr.index&lt;-li.fold[[my.fold]] # index of training \n      x&lt;-li.mat[[\"train\"]][[\"28wk\"]][my.tr.index,] # the training dataset\n      get_lasso_coef(x, my.fold, my.num=my.num) \n    }) %&gt;% rbindlist\n\n    # 3. ElasticNet\n    # warnings(): one multinomial or binomial class has fewer than 8  observations; dangerous ground\n    dt.cv.enet&lt;-lapply(names(li.fold), function(my.fold){\n      message(paste(\"Num:\",my.num,\"Method: ENET, Fold:\",my.fold))\n      my.tr.index&lt;-li.fold[[my.fold]] # index of training \n      x&lt;-li.mat[[\"train\"]][[\"28wk\"]][my.tr.index,] # the training dataset \n      get_enet_coef(x, my.fold, my.num=my.num) \n    }) %&gt;% rbindlist\n\n    # 4. mSVM-RFE\n    dt.cv.svm&lt;-lapply(names(li.fold), function(my.fold){\n      message(paste(\"Num:\",my.num,\"Method: mSVM-RFE, Fold:\",my.fold))\n      my.tr.index&lt;-li.fold[[my.fold]] # index of training \n      x&lt;-li.mat[[\"train\"]][[\"28wk\"]][my.tr.index,] # the training dataset \n      set.seed(333)\n      ranked.features&lt;-colnames(x)[-1][svmRFE(x, k=5, halve.above=100)][1:my.num] # only top x ranked features\n      data.table(method=\"mSVM-RFE\",`fold`=my.fold,feature=ranked.features, score=NA,rank=1:length(ranked.features))\n    }) %&gt;% rbindlist\n\n    # 5-10. Other methods via RFE\n    dt.cv.rfe&lt;-lapply(names(li.methods), function(my.method){\n              # runRFE2 for each repetition\n              lapply(paste0(\"Rep\",1:5), function(iRep){\n                this.fold&lt;-paste(paste0(\"Fold\",1:5),iRep,sep=\".\") # 5-fold for this repetition\n                message(paste(\"Num:\",my.num,\"Method:\",my.method, \"Fold:\",this.fold))\n\n                runRFE2(x=li.mat[[\"train\"]][[\"28wk\"]], my.method=my.method, my.num=my.num, my.index=li.fold[this.fold])\n              }) %&gt;% rbindlist\n    }) %&gt;% rbindlist\n\n    rbind(dt.cv.glp, dt.cv.lasso[method==\"LASSO\"], dt.cv.enet, dt.cv.svm, dt.cv.rfe)\n  }) # end of dl.kcv\n\n  save(dl.kcv, file=my.RData)\n}\n\n##\n##\nmy.RData&lt;-file.path(\"RData/dl.kcv.result.core17.RData\")\nif(file.exists(my.RData)){\n  load(my.RData)\n}else{\n  # for each number of feature: F2, F3, F4, F5, F6\n  dl.kcv.result&lt;-parallel::mclapply(dl.kcv, function(dt.kcv){\n    # features by method and fold\n    dt.foo&lt;-dt.kcv[order(method,fold,feature)][,.(.N,features=paste(feature,collapse=\",\")),.(method,fold)]\n    my.num&lt;-dt.foo[,unique(N)]\n\n    # features by method, fold and features\n    dt.bar&lt;-dt.kcv[order(method,fold,feature)][,.(.N,features=paste(feature,collapse=\",\")),.(method,fold)][,.(.N,methods=paste(method,collapse=\",\")),.(fold,features)][order(fold,-N)]\n\n    # get the CV-LPOCV (or CV-AUC) across 25 folds (i.e. 5-Fold-CV * 5 Rep) \n    dl.bar&lt;-split(dt.bar, dt.bar$fold) # by each fold\n    dt.kcv.result&lt;-parallel::mclapply(dl.bar, function(dt.baz){\n      my.fold&lt;-dt.baz[,.N,fold]$fold\n      my.index&lt;-li.fold[[my.fold]] # index of training\n\n      # for each list of features in this fold\n      lapply(dt.baz$features, function(i){\n        message(paste(\"Num:\",my.num,\", Fold:\",my.fold, \", Features:\",i))\n        my.feature &lt;- i %&gt;% strsplit(\",\") %&gt;% unlist\n        get_cv_glm(x=li.mat[[\"train\"]][[\"28wk\"]],my.fold,my.index,my.feature)\n      }) %&gt;% rbindlist # merge all features\n    },mc.cores=1) %&gt;% rbindlist # merge all fold\n\n    merge(dt.foo, dt.kcv.result, by.x=c(\"fold\",\"features\"), by.y=c(\"fold\",\"predictor\"))\n  },mc.cores=1) # parallel by the No. of features\n  save(dl.kcv.result, file=my.RData)\n}\n\n\nHaving selected a set of predictors for each of the 11 ML methods, a logistic regression model was fitted on the training fold based on the selected predictors, and its predictive performance, i.e. the AUC, was calculated using the remaining held-out test fold. As the 5-fold CV was repeated 5 times, the 25 cross-validated AUCs were averaged by taking the mean AUC. This procedure was repeated from a selection of 2- to 6-predictor models, i.e. 5 times, so the cross-validated AUCs were again averaged by taking the mean values of AUCs.\n\n\nCode to extract the final selected features from each ML method and fit a regression model and test its performance.\nmy.RData&lt;-file.path(\"RData/dl.final.models.core17.RData\")\nif(file.exists(my.RData)){\n  load(my.RData)\n}else{\n  library(e1071)\n  source('SVM-RFE/msvmRFE.R')\n\n  li.methods&lt;-list(\n    `svmLinear`=\"svmLinear\",\n    `svmRadial`=\"svmRadial\",\n    `nnet`=\"nnet\",\n    `pcaNNet`=\"pcaNNet\",\n    `rfFuncs`=rfFuncs,\n    `nbFuncs`=nbFuncs\n    )\n\n  li.num&lt;-list(`F2`=2,`F3`=3,`F4`=4,`F5`=5,`F6`=6)\n  x&lt;-li.mat[[\"train\"]][[\"28wk\"]]\n\n  dl.final.models&lt;-lapply(li.num, function(my.num){\n    # 1. glParallel\n    message(paste(\"Num:\",my.num,\"Method: glParallel\"))\n    my.pattern=paste0(\"^core17\\\\.final\\\\.best\",my.num)\n    dt.input&lt;-data.table(\n                    foo=list.files(\"glParallel/result\",pattern=my.pattern),\n                    files=list.files(\"glParallel/result\",pattern=my.pattern,full.names=T)\n                    )[,c(\"foo1\",\"bar1\"):=tstrsplit(foo,\"\\\\.\",keep=c(2,3))][,fold:=paste(foo1,bar1,sep=\".\")][,c(\"foo\",\"foo1\",\"bar1\"):=NULL]\n\n    dt.glp &lt;-apply(dt.input, 1, function(i){\n              my.fold&lt;-i[[\"fold\"]]\n              my.feature&lt;-fread(i[\"files\"])[1][[\"Best proteins\"]] %&gt;% strsplit(\",\") %&gt;% unlist\n              data.table(method=\"glParallel\",fold=my.fold,feature=my.feature,score=NA,rank=NA)\n              #cbind(`fold`=i[[\"fold\"]],fread(i[\"files\"])[1])\n    }) %&gt;% rbindlist\n\n    # 2. Lasso\n    message(paste(\"Num:\",my.num,\"Method: LASSO\"))\n    dt.lasso&lt;-get_lasso_coef(x=x, \"final\", my.num=my.num)\n\n    # 3. ElasticNet\n    message(paste(\"Num:\",my.num,\"Method: ENET\"))\n    dt.enet&lt;-get_enet_coef(x=x, \"final\", my.num=my.num)\n\n    # 4. mSVM-RFE\n    message(paste(\"Num:\",my.num,\"Method: mSVM-RFE\"))\n    set.seed(333)\n    ranked.features&lt;-colnames(x)[-1][svmRFE(x, k=5, halve.above=100)][1:my.num] # only top 4 ranked features\n    dt.svm&lt;-data.table(method=\"mSVM-RFE\",`fold`=\"final\",feature=ranked.features, score=NA, rank=1:length(ranked.features))\n\n    # 5-10. Other methods via RFE: \"svmLinear\" \"svmRadial\" \"nnet\"      \"pcaNNet\"   \"rfFuncs\"   \"nbFuncs\"\n    dt.rfe&lt;-lapply(names(li.methods), function(my.method){\n              message(paste(\"Num:\",my.num,\"Method:\",my.method))\n              runRFE2(x=x, my.method=my.method, my.num=my.num, my.index=li.fold.final, is.final=T)\n    }) %&gt;% rbindlist\n\n    # compile all the final models #\n    rbind(dt.glp, dt.lasso[method==\"LASSO\"], dt.enet, dt.svm, dt.rfe)[order(method,feature)]\n  }) # end of dl.final.models\n  save(dl.final.models, file=my.RData)\n} # end of if\n\nmy.RData&lt;-file.path(\"RData/dl.final.result.core17.RData\")\nif(file.exists(my.RData)){\n  load(my.RData)\n}else{\n  mat.tr&lt;-li.mat[[\"train\"]][[\"28wk\"]] # training model\n\n  dl.final.result&lt;-lapply(dl.final.models, function(dt.model){\n    dt.final.model&lt;-dt.model[,.(.N,features=paste(feature,collapse=\",\")),method][order(features)]\n    dt.final&lt;-dt.final.model[,.(.N,methods=paste(method,collapse=',')),features]\n\n    #######################################################################\n    # get LPOCV/AUC from the preterm dataset (NB, 28wk-preterm: training) #\n    #######################################################################\n    dt.final.result&lt;-lapply(dt.final$methods, function(my.methods){\n                          ############################################\n                          # fit the model using the training dataset #\n                          ############################################\n                          my.feature&lt;-dt.final[methods==my.methods]$features %&gt;% strsplit(\",\") %&gt;% unlist\n                          df.mat.tr&lt;-mat.tr[,c(my.feature,'y')] %&gt;% as.data.frame  # training set\n                          my.model&lt;-glm(y~. , data = df.mat.tr, family = \"binomial\")\n\n                          ## preterm (NB, 28wk: training dataset where the model was built)\n                          dt.foo1&lt;-lapply(c(\"12wk\",\"20wk\",\"28wk\"), function(my.GA){\n                            message(paste(\"preterm\",my.methods,my.GA,sep=\":\"))\n                            x&lt;-li.mat[[\"train\"]][[my.GA]]\n                            my.fold&lt;-paste0(my.GA,\"(preterm)\")\n                            cbind(`methods`=my.methods,\n                                  get_cv_glm2(x=x,my.fold=my.fold,my.model=my.model,my.feature=my.feature)\n                            )\n                          }) %&gt;% rbindlist\n\n                          ## term (validation)\n                          dt.foo2&lt;-lapply(c(\"12wk\",\"20wk\",\"28wk\",\"36wk\"), function(my.GA){\n                            message(paste(\"term\",my.methods,my.GA,sep=\":\"))\n                            x&lt;-li.mat[[\"test\"]][[my.GA]]\n                            my.fold&lt;-paste0(my.GA,\"(term)\")\n                            cbind(`methods`=my.methods,\n                                  get_cv_glm2(x=x,my.fold=my.fold,my.model=my.model,my.feature=my.feature)\n                            )\n                          }) %&gt;% rbindlist\n                          \n                          ## Munchel \n                          message(paste(\"Munchel\",my.methods,sep=\":\"))\n                          x&lt;-li.mat[[\"munchel\"]]\n                          my.fold&lt;-\"Munchel\"\n                          dt.foo3&lt;-cbind(`methods`=my.methods,\n                                  get_cv_glm2(x=x,my.fold=my.fold,my.model=my.model,my.feature=my.feature)\n                          )\n\n                          rbind(dt.foo1,dt.foo2,dt.foo3)\n                      }) %&gt;% rbindlist\n    dt.final.result&lt;-dt.final.result[order(fold,-AUC_test)]\n    dt.final.result\n  })\n  save(dl.final.result, file=my.RData)\n}",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "cross_validation.html#sec-validation",
    "href": "cross_validation.html#sec-validation",
    "title": "3  Cross Validation",
    "section": "3.5 Internal and external validation",
    "text": "3.5 Internal and external validation\nHaving identified the winning method (as shown in Figure 6.7 A), it was applied to choose 2 to 10 predictors using the whole 28wkGA samples of the discovery dataset and the selected predictors were used to fit multivariate logistic regression models using the same training dataset (Figure 6.7 B). Finally, we evaluated the predictive performance of those 2- to 10-predictor logistic regression models using the term validation cohort and the external Munchel dataset (as shown in Figure 6.8).\n\n\nCode to run internal and external validations for 2-10 predictors chosen by the winning method of 5-fold CV.\nmy.RData&lt;-file.path(\"RData/dl.enet.models.core17.RData\")\nif(file.exists(my.RData)){\n  load(my.RData)\n}else{\n  li.num&lt;-2:10 %&gt;% as.list\n  names(li.num)=paste0(\"F\",2:10)\n\n  x&lt;-li.mat[[\"train\"]][[\"28wk\"]]\n\n  dl.enet.models&lt;-lapply(li.num, function(my.num){\n    # 2. Lasso\n    message(paste(\"Num:\",my.num,\"Method: LASSO\"))\n    dt.lasso&lt;-get_lasso_coef(x=x, \"final\", my.num=my.num)\n\n    # 3. ElasticNet\n    message(paste(\"Num:\",my.num,\"Method: ENET\"))\n    dt.enet&lt;-get_enet_coef(x=x, \"final\", my.num=my.num)\n\n    # compile all the final models #\n    rbind(dt.lasso[method==\"LASSO\"], dt.enet)[order(method,feature)]\n  }) # end of dl.final.models\n  save(dl.enet.models, file=my.RData)\n} # end of if\n\n##\n## ENet and LASSO only using Discovery and Validation datasets\n## \nmy.RData&lt;-file.path(\"RData/dl.enet.result.core17.RData\")\nif(file.exists(my.RData)){\n  load(my.RData)\n}else{\n  mat.tr&lt;-li.mat[[\"train\"]][[\"28wk\"]] # training model\n\n  dl.enet.result&lt;-lapply(dl.enet.models, function(dt.model){\n    dt.final.model&lt;-dt.model[,.(.N,features=paste(feature,collapse=\",\")),method][order(features)]\n    dt.final&lt;-dt.final.model[,.(.N,methods=paste(method,collapse=',')),features]\n    #######################################################################\n    # get LPOCV/AUC from the preterm dataset (NB, 28wk-preterm: training) #\n    #######################################################################\n    dt.final.result&lt;-lapply(dt.final$methods, function(my.methods){\n                          ############################################\n                          # fit the model using the training dataset #\n                          ############################################\n                          my.feature&lt;-dt.final[methods==my.methods]$features %&gt;% strsplit(\",\") %&gt;% unlist\n                          df.mat.tr&lt;-mat.tr[,c(my.feature,'y')] %&gt;% as.data.frame  # training set\n                          my.model&lt;-glm(y~. , data = df.mat.tr, family = \"binomial\")\n\n                          ## preterm (NB, 28wk: training dataset where the model was built)\n                          dt.foo1&lt;-lapply(c(\"12wk\",\"20wk\",\"28wk\"), function(my.GA){\n                            message(paste(\"preterm\",my.methods,my.GA,sep=\":\"))\n                            x&lt;-li.mat[[\"train\"]][[my.GA]]\n                            my.fold&lt;-paste0(my.GA,\"(preterm)\")\n                            cbind(`methods`=my.methods,\n                                  get_cv_glm2(x=x,my.fold=my.fold,my.model=my.model,my.feature=my.feature)\n                            )\n                          }) %&gt;% rbindlist\n\n                          ## term (validation)\n                          dt.foo2&lt;-lapply(c(\"12wk\",\"20wk\",\"28wk\",\"36wk\"), function(my.GA){\n                            message(paste(\"term\",my.methods,my.GA,sep=\":\"))\n                            x&lt;-li.mat[[\"test\"]][[my.GA]]\n                            my.fold&lt;-paste0(my.GA,\"(term)\")\n                            cbind(`methods`=my.methods,\n                                  get_cv_glm2(x=x,my.fold=my.fold,my.model=my.model,my.feature=my.feature)\n                            )\n                          }) %&gt;% rbindlist\n                          \n                          ## Munchel \n                          message(paste(\"Munchel\",my.methods,sep=\":\"))\n                          x&lt;-li.mat[[\"munchel\"]]\n                          my.fold&lt;-\"Munchel\"\n                          dt.foo3&lt;-cbind(`methods`=my.methods,\n                                  get_cv_glm2(x=x,my.fold=my.fold,my.model=my.model,my.feature=my.feature)\n                          )\n\n                          rbind(dt.foo1,dt.foo2,dt.foo3)\n                      }) %&gt;% rbindlist\n    dt.final.result&lt;-dt.final.result[order(fold,-AUC_test)]\n    dt.final.result\n  }) # end of dl.enet.models\n  save(dl.enet.result, file=my.RData)\n}\n\n#\n# The final best models\n#\nmy.RData&lt;-file.path(\"RData/dt.best.result.core17.RData\")\nif(file.exists(my.RData)){\n  load(my.RData)\n}else{\n  my.targets&lt;-c(\"LEP\",\"PAPPA2\",\"LEP,PAPPA2\",\n                \"LEP,LY6G6D,PAPPA2\" # best performing LASSO 3-mRNA model\n                )\n\n  mat.tr&lt;-li.mat[[\"train\"]][[\"28wk\"]] # training model\n  mat.tr&lt;-cbind(mat.tr, \"LEP_PAPPA2\"=mat.tr[,\"LEP\"] * mat.tr[,\"PAPPA2\"])\n\n  #######################################################################\n  # get LPOCV/AUC from the preterm dataset (NB, 28wk-preterm: training) #\n  #######################################################################\n  my.targets&lt;-\"LEP_PAPPA2\"\n\n  dt.best.result&lt;-lapply(my.targets, function(my.proteins){\n                        ############################################\n                        # fit the model using the training dataset #\n                        ############################################\n                        my.feature&lt;-strsplit(my.proteins,\",\") %&gt;% unlist\n                        df.mat.tr&lt;-mat.tr[,c(my.feature,'y')] %&gt;% as.data.frame  # training set\n                        my.model&lt;-glm(y~. , data = df.mat.tr, family = \"binomial\")\n\n                        ## preterm (NB, 28wk: training dataset where the model was built)\n                        dt.foo1&lt;-lapply(c(\"12wk\",\"20wk\",\"28wk\"), function(my.GA){\n                          message(paste(\"preterm\",my.GA,my.proteins,sep=\":\"))\n                          x&lt;-li.mat[[\"train\"]][[my.GA]]\n                          x&lt;-cbind(x, \"LEP_PAPPA2\"=x[,\"LEP\"] * x[,\"PAPPA2\"])\n                          my.fold&lt;-paste0(my.GA,\"(preterm)\")\n                          get_cv_glm2(x=x,my.fold=my.fold,my.model=my.model,my.feature=my.feature)\n                        }) %&gt;% rbindlist\n\n                        ## term (validation)\n                        dt.foo2&lt;-lapply(c(\"12wk\",\"20wk\",\"28wk\",\"36wk\"), function(my.GA){\n                          message(paste(\"term\",my.GA,my.proteins,sep=\":\"))\n                          x&lt;-li.mat[[\"test\"]][[my.GA]]\n                          x&lt;-cbind(x, \"LEP_PAPPA2\"=x[,\"LEP\"] * x[,\"PAPPA2\"])\n                          my.fold&lt;-paste0(my.GA,\"(term)\")\n                          get_cv_glm2(x=x,my.fold=my.fold,my.model=my.model,my.feature=my.feature)\n                        }) %&gt;% rbindlist\n                        \n                        ## Munchel \n                        message(paste(\"Munchel\",my.proteins,sep=\":\"))\n                        x&lt;-li.mat[[\"munchel\"]]\n                        x&lt;-cbind(x, \"LEP_PAPPA2\"=x[,\"LEP\"] * x[,\"PAPPA2\"])\n                        my.fold&lt;-\"Munchel\"\n                        dt.foo3&lt;-get_cv_glm2(x=x,my.fold=my.fold,my.model=my.model,my.feature=my.feature)\n                        rbind(dt.foo1,dt.foo2,dt.foo3)\n                    }) %&gt;% rbindlist\n  save(dt.best.result, file=my.RData)\n}",
    "crumbs": [
      "Methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "SI_select_quant_method.html",
    "href": "SI_select_quant_method.html",
    "title": "4  Selection of RNA-seq quantification method",
    "section": "",
    "text": "4.1 Randomly select 100 samples and downsample reads",
    "crumbs": [
      "Supplementary Text",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Selection of RNA-seq quantification method</span>"
    ]
  },
  {
    "objectID": "SI_select_quant_method.html#sec-si-rand",
    "href": "SI_select_quant_method.html#sec-si-rand",
    "title": "4  Selection of RNA-seq quantification method",
    "section": "",
    "text": "Listing 4.1: downsampling command using bash and seqtk\n\n\n###########################\n# Random sampling for 100 #\n###########################\nfor i in `ls ~/data/fastq/SLX-ILM-Plasma2021/*r_1.fq.gz`; do BARCODE=`echo $i | cut -d/ -f7 | cut -d. -f2`; echo $BARCODE;done | grep -v PPC | sort | uniq -c | awk '$1==4{print $0}' | sort -R | head -n 100 | awk '{print $2}' &gt; ~/results/SLX-ILM-Plasma2021.Homo_sapiens.v1/random.sample.100.txt\njoin -t \",\" &lt;(sort ~/results/SLX-ILM-Plasma2021.Homo_sapiens.v1/random.sample.100.txt) &lt;(sort -t \",\" -k1,1 ~/results/SLX-ILM-Plasma2021.Homo_sapiens.v1/FASTQ.list.r1.txt) &gt; ~/results/SLX-ILM-Plasma2021.Homo_sapiens.v1/random.sample.100.FASTQ.list.r1.txt\n\n######################################\n# Downsampling to 1M for 100 samples #\n######################################\n for i in `awk -F\",\" '{print $2}' ~/results/SLX-ILM-Plasma2021.Homo_sapiens.v1/random.sample.100.FASTQ.list.r1.txt`; do FNAME=SLX-ILM-Plasma2021-ds1M.`echo $i | cut -d/ -f7 | cut -d. -f2,3,4,5,6,7`; echo $i; time seqtk sample -s11 $i 250000 | gzip -9 &gt;  ~/data/fastq/SLX-ILM-Plasma2021-ds1M/$FNAME; done\n for i in `awk -F\",\" '{print $2}' ~/results/SLX-ILM-Plasma2021.Homo_sapiens.v1/random.sample.100.FASTQ.list.r2.txt`; do FNAME=SLX-ILM-Plasma2021-ds1M.`echo $i | cut -d/ -f7 | cut -d. -f2,3,4,5,6,7`; echo $i; time seqtk sample -s11 $i 250000 | gzip -9 &gt;  ~/data/fastq/SLX-ILM-Plasma2021-ds1M/$FNAME; done\n\n######################################\n# Downsampling to 5M for 100 samples #\n######################################\n for i in `awk -F\",\" '{print $2}' ~/results/SLX-ILM-Plasma2021.Homo_sapiens.v1/random.sample.100.FASTQ.list.r1.txt`; do FNAME=SLX-ILM-Plasma2021-ds5M.`echo $i | cut -d/ -f7 | cut -d. -f2,3,4,5,6,7`; echo $i; time seqtk sample -s11 $i 1250000 | gzip -9 &gt;  ~/data/fastq/SLX-ILM-Plasma2021-ds5M/$FNAME; done\n for i in `awk -F\",\" '{print $2}' ~/results/SLX-ILM-Plasma2021.Homo_sapiens.v1/random.sample.100.FASTQ.list.r2.txt`; do FNAME=SLX-ILM-Plasma2021-ds5M.`echo $i | cut -d/ -f7 | cut -d. -f2,3,4,5,6,7`; echo $i; time seqtk sample -s11 $i 1250000 | gzip -9 &gt;  ~/data/fastq/SLX-ILM-Plasma2021-ds5M/$FNAME; done\n\n#######################################\n# Downsampling to 10M for 100 samples #\n###################]###################\n for i in `awk -F\",\" '{print $2}' ~/results/SLX-ILM-Plasma2021.Homo_sapiens.v1/random.sample.100.FASTQ.list.r1.txt`; do FNAME=SLX-ILM-Plasma2021-ds10M.`echo $i | cut -d/ -f7 | cut -d. -f2,3,4,5,6,7`; echo $i; time seqtk sample -s11 $i 2500000 | gzip -9 &gt;  ~/data/fastq/SLX-ILM-Plasma2021-ds10M/$FNAME; done\n for i in `awk -F\",\" '{print $2}' ~/results/SLX-ILM-Plasma2021.Homo_sapiens.v1/random.sample.100.FASTQ.list.r2.txt`; do FNAME=SLX-ILM-Plasma2021-ds10M.`echo $i | cut -d/ -f7 | cut -d. -f2,3,4,5,6,7`; echo $i; time seqtk sample -s11 $i 2500000 | gzip -9 &gt;  ~/data/fastq/SLX-ILM-Plasma2021-ds10M/$FNAME; done",
    "crumbs": [
      "Supplementary Text",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Selection of RNA-seq quantification method</span>"
    ]
  },
  {
    "objectID": "SI_select_quant_method.html#sec-si-quant-salmon",
    "href": "SI_select_quant_method.html#sec-si-quant-salmon",
    "title": "4  Selection of RNA-seq quantification method",
    "section": "4.2 Import downsampled Salmon result",
    "text": "4.2 Import downsampled Salmon result\nThe code below internally calls the prep_salmon function defined in the _libs folder.\n\n\nCode to parse downsampled Salmon results\ndl.chrY.TPM&lt;-list()\nall.ds&lt;-c(\"1M\",\"5M\",\"10M\")\ndl.chrY.TPM&lt;-lapply(all.ds, function(my.ds){\n  message(my.ds)\n  my.RData&lt;-paste0(\"RData/li.TPM.\",my.ds,\".RData\")\n  if(!file.exists(my.RData)){ # read the local output of Salmon \n    my.slx&lt;-ifelse(my.ds==\"1M\",paste0(\"SLX-ILM-Plasma2021-ds\",my.ds,\".Homo_sapiens.v2\"),paste0(\"SLX-ILM-Plasma2021-ds\",my.ds,\".Homo_sapiens.v1\"))\n    li.TPM&lt;-prep_ds_salmon(my.slx) # set li.TPM. See `_libs/local.R` how `li.TPM` was set up\n    save(li.TPM, file=paste0(\"RData/li.TPM.\",my.ds,\".RData\"))\n    message(\"li.TPM saved\")\n  }\n  load(my.RData) %&gt;% system.time\n\n  dt.chrY.TPM&lt;-rbind(\n    data.table(\n      `Type`=\"Salmon (SA mode)\",\n      `SampleID`=li.TPM[[\"Salmon\"]][[\"counts\"]][this.gene,] %&gt;% colSums %&gt;% names,\n      `Counts`=li.TPM[[\"Salmon\"]][[\"counts\"]][this.gene,] %&gt;% colSums, \n      `TPM`=li.TPM[[\"Salmon\"]][[\"TPM\"]][this.gene,] %&gt;% colSums \n      ),\n    data.table(\n      `Type`=\"HiSat2+Salmon\",\n      `SampleID`=li.TPM[[\"Salmon_aln\"]][[\"counts\"]][this.gene,] %&gt;% colSums %&gt;% names,\n      `Counts`=li.TPM[[\"Salmon_aln\"]][[\"counts\"]][this.gene,] %&gt;% colSums,\n      `TPM`=li.TPM[[\"Salmon_aln\"]][[\"TPM\"]][this.gene,] %&gt;% colSums \n      )\n  )\n  # add Sex and GA\n  merge(dt.chrY.TPM, dt.samples[,.(SampleID,GA,Sex,pn_female)])\n})\nnames(dl.chrY.TPM)&lt;-all.ds\n\ndl.stat&lt;-lapply(dl.chrY.TPM, function(i) i[,.(.N,SumCount=sum(Counts),SumTPM=sum(TPM),MeanCount=mean(Counts),MeanTPM=mean(TPM)),.(GA,Sex,Type)][order(GA,Sex)])",
    "crumbs": [
      "Supplementary Text",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Selection of RNA-seq quantification method</span>"
    ]
  },
  {
    "objectID": "SI_select_quant_method.html#number-of-samples-by-ga-and-sex-among-the-100-randomly-selected-samples",
    "href": "SI_select_quant_method.html#number-of-samples-by-ga-and-sex-among-the-100-randomly-selected-samples",
    "title": "4  Selection of RNA-seq quantification method",
    "section": "4.3 Number of samples by GA and sex among the 100 randomly selected samples",
    "text": "4.3 Number of samples by GA and sex among the 100 randomly selected samples\n\n\nCode\n#! label: ds100\nlibrary(magrittr)\ndt.samples&lt;-fread(\"static/R/data/dt.samples.csv\") # n=755\nrand.100&lt;-fread(\"static/R/data/random.sample.100.txt\", header=F) # n=100\nxtabs(~GA+Sex, data=dt.samples[SampleID %in% rand.100$V1]) %&gt;% addmargins\n\n\n      Sex\nGA     Female Male Sum\n  12wk     17    5  22\n  20wk     18    7  25\n  28wk     19   15  34\n  36wk     11    8  19\n  Sum      65   35 100",
    "crumbs": [
      "Supplementary Text",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Selection of RNA-seq quantification method</span>"
    ]
  },
  {
    "objectID": "SI_trimming.html",
    "href": "SI_trimming.html",
    "title": "5  Assessing the effect of trimming the adaptor sequences",
    "section": "",
    "text": "Having compared different approaches of using RNA-seq quantification methods as described above (Chapter 4), we investigated the effect of trimming or no-trimming the adaptor sequences on the performance of predicting fetal sex. For this purpose, we used the 10 million down-sampled reads from the 100 randomly selected samples and ran cutadapt (v2.7 with Python 3.6.8) (11) with the following parameters:\n\n\n\nListing 5.1: cutadapt command\n\n\ncutadapt -j 32 \n         -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCAC \n         -A AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTAGATCTCGGTGGTCGCCGTATCATT \n         -q 20 \n         -O 8 \n         -m 20\n\n\n\nWe employed “Salmon (SA mode)” approach for both trimmed and un-trimmed input reads, then compared the number of read quantified on the 42 chrY genes, and the number of false classifications of fetal sex. Again, a sample is predicted as male if at least one read was quantified on any of the 42 protein-coding chrY genes as described before. We found that, regardless of trimming or not, the number of TP, TN, FP, and FN remained the same and the sum of read quantified on the 42 chrY genes also remained very similar (R=0.999, P&lt;2.2x10-16, Pearson’s correlation test with two-sided, as in Section 7.4) , even though there were two female samples with a slightly higher sum of chrY reads when quantified using trimmed input reads (one 12wkGA sample with a total of 1.887 reads from non-trimmed input and 2.886 from trimmed input; one 20wkGA sample with a total of 0.918 read from non-trimmed input and 0.919 read from trimmed input).\nIn conclusion, we employed “Salmon (SA mode)” without trimming the input raw sequencing reads.",
    "crumbs": [
      "Supplementary Text",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Assessing the effect of trimming the adaptor sequences</span>"
    ]
  },
  {
    "objectID": "main_figure.html",
    "href": "main_figure.html",
    "title": "6  Main Figures",
    "section": "",
    "text": "6.1 Figure 1\nThis figure was made using the Biorender, except some inlet figures (shown further below, e.g. Section 6.2, Section 6.3, Section 6.4, Section 6.5) made by R, and workflow diagrams (e.g. Figure 6.2, Figure 6.3, Figure 6.4, Figure 6.5) written in DOT (Graphic Description Language) such as Graphviz, where you can see some example codes shown below.",
    "crumbs": [
      "Figures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Main Figures</span>"
    ]
  },
  {
    "objectID": "main_figure.html#sec-fig1",
    "href": "main_figure.html#sec-fig1",
    "title": "6  Main Figures",
    "section": "",
    "text": "Figure 6.1: Figure 1 in the paper\n\n\n\n\n\n6.1.1 The dot code in Figure 1A\n\n\nstatic/dot/cfRNA_quant_workflow2.quant.gv\n\ndigraph G {\n  node [fontname = \"Handlee\", color=black, fontcolor=black];\n  edge [fontname = \"Handlee\", color=black, fontcolor=black];\n\n  placenta_rna[\n    label = \"POPS\\nplacenta RNA-Seq\";\n    shape = rect;\n    style = filled;\n    fontcolor = white;\n  ];\n\n  plasma_rna [\n    label = \"POPS\\nplasma RNA-Seq\";\n    shape = rect;\n    style = filled;\n    fontcolor = white;\n  ];\n\n  pops[\n    label = \"POPS placenta\\ntranscriptome\";\n    shape = rect;\n    color = black;\n    fontcolor = black;\n  ];\n\n  quant_pops [\n    label = \"Quantification\";\n    style = \"rounded,filled\";\n    fillcolor = \"darkgoldenrod2\"\n    shape = rect;\n    color = black;\n  ];\n\n  placenta_rna-&gt; pops[label=\"StringTie\"];\n  pops-&gt; quant_pops[label=\"Salmon (index)\"];\n  plasma_rna -&gt; quant_pops[label=\"Salmon (SA-mode)\"]\n\n  {rank=same; placenta_rna plasma_rna} \n}\n\n\n\n\n\n\n\n\n\nG\n\n\n\nplacenta_rna\n\nPOPS\nplacenta RNA-Seq\n\n\n\npops\n\nPOPS placenta\ntranscriptome\n\n\n\nplacenta_rna-&gt;pops\n\n\nStringTie\n\n\n\nplasma_rna\n\nPOPS\nplasma RNA-Seq\n\n\n\nquant_pops\n\nQuantification\n\n\n\nplasma_rna-&gt;quant_pops\n\n\nSalmon (SA-mode)\n\n\n\npops-&gt;quant_pops\n\n\nSalmon (index)\n\n\n\n\n\n\nFigure 6.2: A diagram of cfRNA quantification\n\n\n\n\n\n\n\n6.1.2 The dot code in Figure 1B\n\n\nstatic/dot/cfRNA_quant_workflow2.discovery.gv\n\ndigraph G {\n  /*rankdir=LR; */\n  node [fontname = \"Handlee\", color=black, fontcolor=black];\n  edge [fontname = \"Handlee\", color=black, fontcolor=black];\n\n  quant_pops [\n    label = \"Quantification\";\n    shape = rect;\n    style = filled;\n    fontcolor = white;\n  ];\n\n  deg_pops [\n    label = \"DEG\";\n    shape = rect;\n  ];\n\n  core_deg[\n    label = \"Core DEG\";\n    shape = rect;\n  ];\n\n  best_model [\n    label = \"Best model\";\n    style = \"rounded,filled\";\n    fillcolor = \"darkgoldenrod2\"\n    shape = rect;\n    color = black;\n  ];\n\n  check_top1[\n    label = \"Top1% DEG\\nby all methods\";\n    shape = diamond;\n  ];\n\n  quant_pops-&gt; deg_pops[label=\"DESeq2\"];\n  quant_pops-&gt; deg_pops[label=\"edgeR\"];\n  quant_pops-&gt; deg_pops[label=\"Logistic\\nRegression\"];\n\n  deg_pops-&gt; check_top1;\n  check_top1-&gt; core_deg;\n  core_deg-&gt; best_model [label=\"5-fold CV\\n11 ML methods\"];\n\n/*\n  {rank=same; check_top1  core_deg} \n  {rank=same; best_model deg_pops} \n*/\n}\n\n\n\n\n\n\n\n\n\nG\n\n\n\nquant_pops\n\nQuantification\n\n\n\ndeg_pops\n\nDEG\n\n\n\nquant_pops-&gt;deg_pops\n\n\nDESeq2\n\n\n\nquant_pops-&gt;deg_pops\n\n\nedgeR\n\n\n\nquant_pops-&gt;deg_pops\n\n\nLogistic\nRegression\n\n\n\ncheck_top1\n\nTop1% DEG\nby all methods\n\n\n\ndeg_pops-&gt;check_top1\n\n\n\n\n\ncore_deg\n\nCore DEG\n\n\n\nbest_model\n\nBest model\n\n\n\ncore_deg-&gt;best_model\n\n\n5-fold CV\n11 ML methods\n\n\n\ncheck_top1-&gt;core_deg\n\n\n\n\n\n\n\n\nFigure 6.3: A diagram of finding the best predictive model in the discovery cohort\n\n\n\n\n\n\n\n6.1.3 The dot code in Figure 1C\n\n\nstatic/dot/cfRNA_quant_workflow2.validation.gv\n\ndigraph G {\n  /*rankdir=LR; */\n  node [fontname = \"Handlee\", color=black, fontcolor=black];\n  edge [fontname = \"Handlee\", color=black, fontcolor=black];\n\n  quant_pops [\n    label = \"Quantification\\n(Validation)\";\n    shape = rect;\n    style = filled;\n    fontcolor = white;\n  ];\n\n  best_model [\n    label = \"Best model\\n(Discovery)\";\n    shape = rect;\n    style = filled;\n    fontcolor = white;\n  ];\n\n  predict [\n    label = \"Predict the outcome\\nof validation dataset\";\n    shape = rect;\n  ];\n\n  score [\n    label = \"Predictive performance\\nof best model\";\n    style = \"rounded,filled\";\n    fillcolor = \"darkgoldenrod2\"\n    shape = rect;\n    color = black;\n  ];\n\n  quant_pops -&gt; predict [label=\"by each gestation\"];\n  best_model -&gt; predict;\n  predict -&gt; score;\n\n/*  {rank=same; predict best_model} */\n}\n\n\n\n\n\n\n\n\n\nG\n\n\n\nquant_pops\n\nQuantification\n(Validation)\n\n\n\npredict\n\nPredict the outcome\nof validation dataset\n\n\n\nquant_pops-&gt;predict\n\n\nby each gestation\n\n\n\nbest_model\n\nBest model\n(Discovery)\n\n\n\nbest_model-&gt;predict\n\n\n\n\n\nscore\n\nPredictive performance\nof best model\n\n\n\npredict-&gt;score\n\n\n\n\n\n\n\n\nFigure 6.4: A diagram showing the validation of best predictive model in the validation cohort\n\n\n\n\n\n\n\n6.1.4 The dot code in Figure 1D\n\n\nstatic/dot/cfRNA_quant_workflow2.munchel.gv\n\ndigraph G {\n  /*rankdir=LR; */\n  node [fontname = \"Handlee\", color=black, fontcolor=black];\n  edge [fontname = \"Handlee\", color=black, fontcolor=black];\n\n  quant_munchel [\n    label = \"Quantification\\n(Munchel)\";\n    shape = rect;\n    style = filled;\n    fontcolor = white;\n  ];\n\n  best_model [\n    label = \"Best model\\n(Discovery)\";\n    shape = rect;\n    style = filled;\n    fontcolor = white;\n  ];\n\n  predict [\n    label = \"Predict the outcome\\nof Munchel dataset\";\n    shape = rect;\n  ];\n\n  score [\n    label = \"Predictive performance\\nof best model\";\n    style = \"rounded,filled\";\n    fillcolor = \"darkgoldenrod2\"\n    shape = rect;\n    color = black;\n  ];\n\n  quant_munchel -&gt; predict \n  best_model -&gt; predict;\n  predict -&gt; score;\n\n/*  {rank=same; predict best_model} */\n}\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nquant_munchel\n\nQuantification\n(Munchel)\n\n\n\npredict\n\nPredict the outcome\nof Munchel dataset\n\n\n\nquant_munchel-&gt;predict\n\n\n\n\n\nbest_model\n\nBest model\n(Discovery)\n\n\n\nbest_model-&gt;predict\n\n\n\n\n\nscore\n\nPredictive performance\nof best model\n\n\n\npredict-&gt;score\n\n\n\n\n\n\n A diagram showing the validation of best predictive model using the external data \n\n\n\n\n\nFigure 6.5",
    "crumbs": [
      "Figures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Main Figures</span>"
    ]
  },
  {
    "objectID": "main_figure.html#sec-fig2",
    "href": "main_figure.html#sec-fig2",
    "title": "6  Main Figures",
    "section": "6.2 Figure 2",
    "text": "6.2 Figure 2\n\n\n\n\n\n\nFigure 6.6: Figure 2 in the paper\n\n\n\n\n6.2.1 Figure 2A\n\n\nR code to generate a histogram of p-values using ggplot\n# cfRNA.preterm.PE.modelling.logitZ.POPS-2022.GRCh38.88.Rmd\n# fig2B. histogram of p-values from univariable logistic regression using the 28wk discovery\n  load(\"RData/dl.logregZ.preterm.POPS-2022.GRCh38.88.RData\")\n\n  sapply(dl.logregZ, function(DT) DT[padj.BH&lt;0.05] %&gt;% nrow)\n\n  #ks.test(dl.logregZ[[\"28wk\"]]$pval,\"punif\",0,1) #,exact=T)\n\n  p2.hist&lt;-ggplot(dl.logregZ[[\"28wk\"]],aes(pval)) +\n         geom_histogram(aes(y=..density..),alpha=.8,bins=40) +\n         #geom_density(alpha=.5) +\n         xlab(\"P-value\") + ylab(\"Density\") +\n         annotate(\"text\",x=0.48,y=10,label=\"P-value &lt; 2.2e-16\",size=6) +\n         theme_Publication()\n  print(p2.hist)\n\n\n\n\n6.2.2 Figure 2B\n\n\nCode\n  load(\"RData/dl.res.edgeR.preterm.POPS-2022.GRCh38.88.RData\") # edgeR result\n  dl.res.edgeR[[\"28wk\"]]\n\n## cfRNA.preterm.PE.modelling.DEG.POPS-2022.GRCh38.88.Rmd\n## fig1A. Volcano (28wk) - edgeR (using original p-values)\n  res.edger&lt;-dl.res.edgeR[[\"28wk\"]]\n\n  (dt.label&lt;-\n  rbind(\n  res.edger[order(BH)][BH&lt;0.05,.SD[c(1:6)]],\n  res.edger[order(-abs(logFC))][,.SD[c(1:2)]]\n  ) %&gt;% unique)\n\n  p2.vol&lt;-ggplot(res.edger, aes(logFC, -log10(PValue))) + \n        geom_point(data=res.edger[BH&gt;0.05],size=2, col=\"grey90\", alpha=.6) +\n        geom_point(data=res.edger[BH&lt;=0.05], size=2,col=\"grey20\",alpha=.8) +\n        ylab(\"-log10(P-value)\") + xlab(\"log2(Fold change)\") +\n        ggrepel::geom_text_repel(\n                                data=res.edger[order(BH)][BH&lt;0.05,.SD[c(1:6)]] # dt.label\n                                ,aes(label=gene_id),\n                                box.padding = 0.6, max.overlaps = Inf,\n                                #direction='gy',\n                                #nudge_y=1,\n                                size=5) +\n        theme_Publication()\n  print(p2.vol)\n\n\n\n\n6.2.3 Figure 2C\n\n\nCode\n# cfRNA.preterm.PE.modelling.logitZ.POPS-2022.GRCh38.88.Rmd\n  load(\"RData/dl.resLFC.preterm.POPS-2022.GRCh38.88.RData\")\n\n  venn.top1pctZ&lt;-list(\n                   `edgeR`=dl.res.edgeR[[\"28wk\"]][order(PValue)][1:151]$gene_id,\n                   `DESeq2`=dl.resLFC[[\"28wk\"]][order(pvalue)][1:151]$gene_id,\n                  `AUC`=dl.logregZ[[\"28wk\"]][order(-auc)][1:151]$gene_id,\n                  `AIC`=dl.logregZ[[\"28wk\"]][order(AIC)][1:151]$gene_id\n                  )\n  p2.venn&lt;-ggvenn::ggvenn(venn.top1pctZ,fill_color=rep(\"grey100\",length(venn.top1pctZ)),set_name_size=5.6,text_size=6,stroke_size=.7,show_percentage=FALSE)\n  print(p2.venn)\n\n  dt.venn.top1pctZ&lt;-lapply(names(venn.top1pctZ), function(i) data.table(i,venn.top1pctZ[[i]])) %&gt;% rbindlist %&gt;% dcast.data.table(V2~i, fun=length)\n  setnames(dt.venn.top1pctZ,\"V2\",\"Gene\")\n  top1pct.genesZ&lt;-dt.venn.top1pctZ[AIC==1 & AUC==1 & DESeq2==1 & edgeR==1]$Gene\n\n\n\n\n6.2.4 Figure 2D\n\n\nCode\n## Heatmap of the 17 core genes\n# cfRNA.preterm.PE.modelling.WF2.POPS-2022.GRCh38.88.Rmd \n  load(\"RData/dt.cpmZ.preterm.POPS-2022.GRCh38.88.RData\")\n\n  #top1pct.genesZ # the 17 core DEGs\n  this.mat&lt;-dt.cpmZ[GA==\"28wk\" & geneName %in% top1pct.genesZ,.(SampleID,geneName,logCPMZ,y=ifelse(Condition==\"Case\",1,0))] %&gt;% dcast.data.table(SampleID+y~geneName,value.var=\"logCPMZ\") %&gt;% as.matrix(rownames=\"SampleID\") # %&gt;% as.data.frame\n  #pheatmap::pheatmap(t(this.mat[,top1pct.genesZ]))\n\n  p.anno&lt;-HeatmapAnnotation(outcome=ifelse(this.mat[,\"y\"]==1,\"case\",\"control\"),\n                            which=\"column\", \n                            col=list(outcome=c(`case`=\"grey10\",`control`=\"grey90\"))\n                        )\n  p.main&lt;-ComplexHeatmap::Heatmap(t(this.mat[,top1pct.genesZ]),\n                        name='z-score',\n                        row_names_gp=gpar(fontsize=10),\n                        show_column_names =F,\n                        #col=circlize::colorRamp2(c(min(this.mat,na.rm=T), 0, max(this.mat,na.rm=T)), c(\"blue\", \"white\", \"red\")),\n                        top_annotation=p.anno\n                        )\n  p2.heat = grid.grabExpr(ComplexHeatmap::draw(p.main,merge_legend=T))\n  print(p2.heat)\n\n\n\n\n6.2.5 Figure 2A-D\n\n\nCode\n## p2.vol\n## p2.hist\n## p2.venn\n## p2.heatmap\n  pdf(file=\"Figures/cfRNA.Fig2.pdf\", width=11, height=9,title=\"Fig2: the 17 core genes\")\n  cowplot::plot_grid(p2.hist, p2.vol, p2.venn, p2.heat, nrow=2,labels=\"AUTO\",label_size=27,align=\"hv\",axis='l')\n  dev.off()",
    "crumbs": [
      "Figures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Main Figures</span>"
    ]
  },
  {
    "objectID": "main_figure.html#sec-fig3",
    "href": "main_figure.html#sec-fig3",
    "title": "6  Main Figures",
    "section": "6.3 Figure 3",
    "text": "6.3 Figure 3\n\n\n\n\n\n\nFigure 6.7: Figure 3 in the paper\n\n\n\n\n6.3.1 Figure 3A\n\n\nCode\n  load(\"RData/dl.kcv.result.core17.RData\")\n  #########################################################\n  # Reulst of 11 ML methods based on 5-fold CV with 5 rep #\n  #########################################################\n  (dt.foo&lt;-lapply(dl.kcv.result, function(DT) DT[,.(.N, \n      mean_AUC_test=mean(AUC_test), mean_AUC_test_lo=mean(AUC_test_lo), mean_AUC_test_hi=mean(AUC_test_hi)\n      ),method][,Rank:=frank(-mean_AUC_test)][order(Rank)]\n  ) %&gt;% rbindlist )\n  (dt.bar&lt;-dt.foo[,.(.N,N_test=sum(N),rankSum=sum(Rank),meanRank=sum(Rank)/5, #F2-F6\n            meanAUC=mean(mean_AUC_test)/100,\n            meanAUC_lo=mean(mean_AUC_test_lo)/100,\n            meanAUC_hi=mean(mean_AUC_test_hi)/100)\n  ,method][order(rankSum)])\n\n  dt.bar[,.(`Method`=method,`Rank Sum`=rankSum,`Mean Rank`=meanRank,`Mean AUC`=meanAUC,`AUC Range`=\"\")] %&gt;%\n  kbl(booktabs=F, digits=4) %&gt;%\n  kable_styling(latex_options = c(\"basic\"),full_width=F,font_size=10) %&gt;%\n  #kable_styling(bootstrap_options = c(\"condensed\"), full_width = F, font_size=10) %&gt;%\n  column_spec(5, image = spec_pointrange(\n    x = dt.bar$meanAUC,\n    xmin = dt.bar$meanAUC_lo,\n    xmax = dt.bar$meanAUC_hi,\n    cex= 0.6,\n    vline = .80,\n    height=50)\n  ) %&gt;% save_kable(\"Figures/cfRNA.Fig2.KCV.table.pdf\")\n\n  ##\n  ##\n  (dt.foo&lt;-lapply(names(dl.kcv.result), function(NF) \n      dl.kcv.result[[NF]][,.(NF=substr(NF,2,3),.N, \n      mean_AUC=mean(AUC_test)/100, mean_AUC_lo=mean(AUC_test_lo)/100, mean_AUC_hi=mean(AUC_test_hi)/100\n      ),method][,Rank:=frank(-mean_AUC)][order(Rank)]\n  ) %&gt;% rbindlist )\n  dt.bar[,.(Method=method,`Rank sum`=rankSum, `Mean rank`=meanRank,round(meanAUC,3),round(meanAUC_lo,3),round(meanAUC_hi,3))][,.(Method,paste(V4,\"(\",V5,\"-\",V6,\")\"))]\n  (dt.baz&lt;-dt.bar[,.(Method=method,`Rank sum`=rankSum, `Mean rank`=meanRank,`Mean AUC (95% CI)`=paste0(sprintf(\"%.3f\",meanAUC),\" (\",round(meanAUC_lo,3),\" - \",round(meanAUC_hi,3),\")\"))])\n\n  library(ggpmisc)\n\n  p.kcv&lt;-ggplot(dt.foo, aes(method, mean_AUC)) +\n    geom_pointrange(aes(col=NF, ymin=mean_AUC_lo, ymax=mean_AUC_hi),position=position_dodge(width=0.7), size=.9,alpha=.8) +\n    scale_x_discrete(limits=dt.bar$method) +\n    scale_y_continuous(expand=c(0,0),breaks=c(0,.2,.4,.6,.8,1), limit=c(0,1.05)) +\n    ggsci::scale_color_jco(name=\"Number of mRNA in the training model\") +\n    labs(x=\"ML Method\",y=\"The mean AUC \\nacross 5-fold CV with 5 repetitions\") +\n    geom_table(data=data.table(x=\"LASSO\", y=0.03, tb=list(dt.baz)),aes(x=x,y=y, label=tb), size=5, table.theme = ttheme_gtlight ) +\n    theme_Publication() +\n    theme(legend.position=\"top\", \n        legend.key = element_rect(fill = \"transparent\"),\n        legend.background = element_rect(fill='transparent', linetype=\"solid\",color=\"black\",size=.2),\n        legend.box.background = element_rect(fill='transparent'),\n        axis.text.x = element_text(angle = 45, hjust=1, size=rel(1)))\n\n\n\n\n6.3.2 Figure 3B\n\n\nCode\n  load(\"RData/dl.final.result.core17.RData\")\n  load(\"RData/dl.enet.result.core17.RData\")\n  load(\"RData/dt.best.result.core17.RData\") # dt.best.result\n library(ggbreak) \n\n  ###############################################\n  # The final training models from LASSO & ENet #\n  ###############################################\n  (dt.enet.tr&lt;-lapply(names(dl.final.result), function(FN){\n    dl.final.result[[FN]][grepl(\"ENet\",methods) & fold==\"28wk(preterm)\",.(Method=\"ENet\",FN=FN,predictor,AUC=AUC_test/100,AUC_lo=AUC_test_lo/100,AUC_hi=AUC_test_hi/100,LPOCV=LPOCV_test/100,LPOCV_lo=LPOCV_test_lo/100,LPOCV_hi=LPOCV_test_hi/100)]\n  }) %&gt;% rbindlist)\n  (dt.lasso.tr&lt;-lapply(names(dl.final.result), function(FN){\n    dl.final.result[[FN]][grepl(\"LASSO\",methods) & fold==\"28wk(preterm)\",.(Method=\"LASSO\",FN=FN,predictor,AUC=AUC_test/100,AUC_lo=AUC_test_lo/100,AUC_hi=AUC_test_hi/100,LPOCV=LPOCV_test/100,LPOCV_lo=LPOCV_test_lo/100,LPOCV_hi=LPOCV_test_hi/100)]\n  }) %&gt;% rbindlist)\n  dt.final.tr&lt;-rbind(dt.enet.tr, dt.lasso.tr)\n\n  dt.final.tr[,.(Method,`Selected features`=predictor,AUC,`AUC Range`=\"\",LPOCV,`LPOCV Range`=\"\")]\n\n  ##################################\n  ## Train for ENet and LASSO only #\n  ##################################\n  lapply(names(dl.enet.result), function(FN)\n    dl.enet.result[[FN]][grepl(\"ENet\",methods) & fold==\"28wk(preterm)\",-c(\"fold\")][,`:=`(Method=\"ENet\",`N`=substr(FN,2,nchar(FN)) %&gt;% as.integer)]\n  ) %&gt;% rbindlist\n\n  lapply(names(dl.enet.result), function(FN)\n    dl.enet.result[[FN]][grepl(\"LASSO\",methods) & fold==\"28wk(preterm)\",-c(\"fold\")][,`:=`(Method=\"LASSO\",`N`=substr(FN,2,nchar(FN)) %&gt;% as.integer)]\n  ) %&gt;% rbindlist\n\n  dt.enet.lasso.tr&lt;-lapply(names(dl.enet.result), function(FN){\n    dt.foo&lt;-dl.enet.result[[FN]][grepl(\"ENet\",methods) & fold==\"28wk(preterm)\",-c(\"fold\")][,`:=`(Method=\"ENet\",`N`=substr(FN,2,nchar(FN)) %&gt;% as.integer)]\n    dt.bar&lt;-dl.enet.result[[FN]][grepl(\"LASSO\",methods) & fold==\"28wk(preterm)\",-c(\"fold\")][,`:=`(Method=\"LASSO\",`N`=substr(FN,2,nchar(FN))%&gt;% as.integer)]\n\n    rbind(\n    dt.foo[,.(Method,N,predictor,Measure=\"AUC\",Score=AUC_test/100,Lo=AUC_test_lo/100,Hi=AUC_test_hi/100)],\n    dt.foo[,.(Method,N,predictor,Measure=\"LPOCV-AUC\",Score=LPOCV_test/100,Lo=LPOCV_test/100,Hi=LPOCV_test/100)],\n    #dt.foo[,.(Method,N,predictor,Measure=\"LPOCV-AUC\",Score=LPOCV_test/100,Lo=LPOCV_test_lo/100,Hi=LPOCV_test_hi/100)],\n    dt.foo[,.(Method,N,predictor,Measure=\"BIC\",Score=BIC,Lo=BIC,Hi=BIC)],\n    dt.foo[,.(Method,N,predictor,Measure=\"AIC\",Score=AIC,Lo=AIC,Hi=AIC)],\n    dt.bar[,.(Method,N,predictor,Measure=\"AUC\",Score=AUC_test/100,Lo=AUC_test_lo/100,Hi=AUC_test_hi/100)],\n    dt.bar[,.(Method,N,predictor,Measure=\"LPOCV-AUC\",Score=LPOCV_test/100,Lo=LPOCV_test/100,Hi=LPOCV_test/100)],\n    #dt.bar[,.(Method,N,predictor,Measure=\"LPOCV-AUC\",Score=LPOCV_test/100,Lo=LPOCV_test_lo/100,Hi=LPOCV_test_hi/100)],\n    dt.bar[,.(Method,N,predictor,Measure=\"BIC\",Score=BIC,Lo=BIC,Hi=BIC)],\n    dt.bar[,.(Method,N,predictor,Measure=\"AIC\",Score=AIC,Lo=AIC,Hi=AIC)]\n    )\n  }\n  ) %&gt;% rbindlist\n\n  p.enet.lasso.tr&lt;-ggplot(dt.enet.lasso.tr[Method==\"ENet\"], aes(N,Score,group=Measure)) +\n    geom_pointrange(aes(col=Measure,ymin=Lo, ymax=Hi),position=position_dodge(width=0.2), size=.9,alpha=.8) +\n    geom_line(aes(col=Measure),size=1,alpha=.8) +\n    scale_x_discrete(limits=2:10) +\n    scale_y_continuous(limits=c(0.7,max(dt.enet.lasso.tr$Score+10)),expand=c(0,0)) +\n    scale_color_manual(values=c(cbPalette[4],cbPalette[3],\"grey30\",cbPalette[2]), breaks=c(\"BIC\",\"AIC\",\"AUC\",\"LPOCV-AUC\")) +\n    labs(x=\"Number of predictor mRNA in the training model\",y=\"Score\") +\n    #facet_wrap(Method~.,ncol=2,scales=\"free_y\",strip.position=\"top\") +\n    ggbreak::scale_y_break(c(1, 25),scales=\"free\") +\n    theme_Publication() +\n    theme(legend.position=\"top\",\n      legend.key = element_rect(fill = \"transparent\"),\n      legend.background = element_rect(fill='transparent', linetype=\"solid\",color=\"black\",size=.2),\n      legend.box.background = element_rect(fill='transparent'),\n      axis.text.y.right = element_blank(),\n      axis.ticks.y.right = element_blank(),\n      axis.line.y.right = element_blank(),\n      plot.margin=unit(c(-3,0.2,0.2,0.2),\"cm\") \n    )\n\n\n\n\n6.3.3 Figure 3A-B\n\n\nCode\n  ##\n  library(patchwork)\n  #\n  pdf(file=\"Figures/cfRNA.KCV.ENet.tr.pdf\", width=11, height=17, title=\"ENet AUC on validation dataset\")\n  p.kcv / p.enet.lasso.tr + plot_annotation(tag_levels = 'A') + plot_layout(heights=c(1.3,1)) & theme(plot.tag = element_text(size = 27, face=\"bold\"))\n  dev.off()",
    "crumbs": [
      "Figures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Main Figures</span>"
    ]
  },
  {
    "objectID": "main_figure.html#sec-fig4",
    "href": "main_figure.html#sec-fig4",
    "title": "6  Main Figures",
    "section": "6.4 Figure 4",
    "text": "6.4 Figure 4\n\n\n\n\n\n\nFigure 6.8: Figure 4 in the paper\n\n\n\n\n6.4.1 Figure 4A\n\n\nCode\n  #########################################\n  ## Validation result using Enet & LASSO #\n  #########################################\n  dt.enet.lasso.val&lt;-lapply(names(dl.enet.result), function(FN){\n    rbind(\n    dl.enet.result[[FN]][grepl(\"ENet\",methods),.(FN=FN,Method=\"ENet\",predictor,fold,AUC=AUC_test/100,AUC_lo=AUC_test_lo/100,AUC_hi=AUC_test_hi/100)],\n    dl.enet.result[[FN]][grepl(\"LASSO\",methods),.(FN=FN,Method=\"LASSO\",predictor,fold,AUC=AUC_test/100,AUC_lo=AUC_test_lo/100,AUC_hi=AUC_test_hi/100)]\n    )\n  }) %&gt;% rbindlist\n  dt.enet.lasso.val[,FN:=substr(FN,2,length(FN))]\n  dt.enet.lasso.val$FN&lt;-factor(dt.enet.lasso.val$FN,level=2:10)#&lt;- # isa `char`\n  dt.enet.lasso.val[,Source:=ifelse(fold==\"Munchel\",fold,ifelse(grepl(\"preterm\",fold),\"Discovery\",\"Validation\"))]\n  xtabs(~fold+Source+Method, dt.enet.lasso.val)\n\n  # Mean validated AUC by GA and Method\n  dt.enet.lasso.val[fold==\"12wk(term)\"][order(-AUC)][,.(.N,Mean=mean(AUC),SD=sd(AUC),Max=max(AUC),Min=min(AUC)),.(Method)]\n  dt.enet.lasso.val[fold==\"20wk(term)\"][order(-AUC)][,.(.N,Mean=mean(AUC),SD=sd(AUC),Max=max(AUC),Min=min(AUC)),.(Method)]\n  dt.enet.lasso.val[fold==\"28wk(term)\"][order(-AUC)][,.(.N,Mean=mean(AUC),SD=sd(AUC),Max=max(AUC),Min=min(AUC)),.(Method)]\n  dt.enet.lasso.val[fold==\"36wk(term)\"][order(-AUC)][,.(.N,Mean=mean(AUC),SD=sd(AUC),Max=max(AUC),Min=min(AUC)),.(Method)]\n\n  # Mean validated AUC by GA merging the two methods\n  dt.enet.lasso.val[fold==\"12wk(term)\"][order(-AUC)][,.(.N,Mean=mean(AUC),SD=sd(AUC),Max=max(AUC),Min=min(AUC))]\n  dt.enet.lasso.val[fold==\"20wk(term)\"][order(-AUC)][,.(.N,Mean=mean(AUC),SD=sd(AUC),Max=max(AUC),Min=min(AUC))]\n  dt.enet.lasso.val[fold==\"28wk(term)\"][order(-AUC)][,.(.N,Mean=mean(AUC),SD=sd(AUC),Max=max(AUC),Min=min(AUC))]\n  dt.enet.lasso.val[fold==\"36wk(term)\"][order(-AUC)][,.(.N,Mean=mean(AUC),SD=sd(AUC),Max=max(AUC),Min=min(AUC))]\n\n  # Best performing model by GA\n  dt.enet.lasso.val[Source==\"Validation\" & Method==\"ENet\",.(Method,fold,FN,predictor,AUC)][order(fold,-AUC)][,.SD[1],fold]\n  dt.enet.lasso.val[Source==\"Validation\" & Method==\"LASSO\",.(Method,fold,FN,predictor,AUC)][order(fold,-AUC)][,.SD[1],fold]\n\n  # get the best performing model by Lasso and ENet separately\n  dt.enet.lasso.val[Source==\"Validation\"][order(Method,fold,-AUC)][,.SD[1],.(Method,fold)] \n\n  #\n  dt.foo&lt;-dt.enet.lasso.val[Source==\"Validation\"][,GA:=substr(fold,1,4)]\n  p.enet.lasso.val&lt;-ggplot(dt.foo, aes(GA, AUC,col=Method)) + \n    geom_pointrange(aes(ymin=AUC_lo, ymax=AUC_hi),position=position_dodge(width=0.7), size=.9,alpha=.8) +\n    scale_color_manual(values=c(\"grey30\",\"grey60\")) +\n    scale_y_continuous(expand=c(0,0),breaks=c(0,.2,.4,.6,.8,1), limit=c(0,1.05)) +\n    ylab(\"AUC\") + xlab(\"Gestational age (validation dataset)\") +\n    facet_wrap(~FN,nrow=1) +\n    theme_Publication() + \n    theme(legend.position=\"top\",\n          axis.text.x = element_text(angle = 45, hjust=1, size=rel(.8)))\n  #\n  p.enet.lasso.val2&lt;-ggplot(dt.foo, aes(FN, AUC,col=Method)) + \n    geom_pointrange(aes(ymin=AUC_lo, ymax=AUC_hi),position=position_dodge(width=0.8), size=.8,alpha=.8) +\n    scale_color_manual(values=c(\"grey30\",\"grey60\")) +\n    scale_y_continuous(expand=c(0,0),breaks=c(0,.2,.4,.6,.8,1), limit=c(0,1.05)) +\n    ylab(\"AUC\") + xlab(\"Number of predictor mRNA in the training model\") +\n    facet_grid(Source~GA) + \n    theme_Publication() + \n    theme(legend.position=\"top\")\n  #\n  dt.foo&lt;-dt.enet.lasso.val[Source==\"Validation\" & Method==\"ENet\"][,GA:=substr(fold,1,4)]\n  p4.enet.lasso.val3&lt;-ggplot(dt.foo, aes(FN, AUC)) + \n    geom_pointrange(aes(ymin=AUC_lo, ymax=AUC_hi), size=.8,alpha=.8, col=\"grey30\") +\n    scale_y_continuous(expand=c(0,0),breaks=c(0,.2,.4,.6,.8,1), limit=c(0,1.05)) +\n    ylab(\"AUC\") + xlab(\"Number of predictor mRNA in the training model\") +\n    facet_grid(Source~GA) + \n    theme_Publication() +\n    theme(legend.position=\"top\", panel.border = element_rect(colour = \"black\"))\n\n  p4.enet.lasso.val4&lt;-ggplot(dt.foo, aes(FN, AUC)) + \n    geom_pointrange(aes(ymin=AUC_lo, ymax=AUC_hi,col=GA), position=position_dodge(width=.8), size=.8,alpha=.8) +\n    scale_y_continuous(expand=c(0,0),breaks=c(0,.2,.4,.6,.8,1), limit=c(0,1.05)) +\n    ggsci::scale_color_jama() +\n    ylab(\"AUC\") + xlab(\"Number of predictor mRNA in the training model\") +\n    facet_grid(Source~.) + \n    theme_Publication() +\n    theme(legend.position=\"top\", panel.border = element_rect(colour = \"black\"))\n\n\n\n\n6.4.2 Figure 4B\n\n\nCode\n  ########################################################################\n  ## Two univariable mRNA models (LEP and PAPPA2) + 2- and 3-mRNA models #\n  ########################################################################\n  #dt.best.result[predictor==\"LEP,LY6G6D,PAPPA2\",predictor:=\"LEP,PAPPA2,LY6G6D\"] # rename\n  dt.best.result&lt;-dt.best.result[predictor!=\"LEP,LY6G6D,PAPPA2\"] # remove these rows\n  dt.best.result[,Source:=ifelse(fold==\"Munchel\",fold,ifelse(grepl(\"preterm\",fold),\"Discovery\",\"Validation\"))]\n\n  dt.best.result[Source==\"Discovery\"]\n  dt.best.result[Source==\"Validation\"]\n  dt.best.result[Source==\"Munchel\"][order(-AUC_test)]\n\n  dt.baz&lt;-dt.best.result[Source!=\"Munchel\",.(Source,fold,predictor,AUC=AUC_test/100,AUC_lo=AUC_test_lo/100,AUC_hi=AUC_test_hi/100)][,GA:=substr(fold,1,4)]\n  p4.top.model&lt;-ggplot(dt.baz, aes(predictor, AUC)) + \n    geom_rect(data = dt.baz[Source==\"Discovery\" & GA==\"28wk\"],aes(fill = GA),fill=\"grey80\",xmin = -Inf,xmax = Inf, ymin = -Inf,ymax = Inf,alpha = 0.2) +\n    geom_pointrange(aes(ymin=AUC_lo, ymax=AUC_hi),col=\"grey30\",size=.9,alpha=.8) +\n    scale_x_discrete(limits=dt.baz[,.N,predictor]$predictor) +\n    scale_y_continuous(expand=c(0,0),breaks=c(0,.2,.4,.6,.8,1), limit=c(0,1.05)) +\n    ylab(\"AUC\") + xlab(\"mRNA(s) in the predictive models\") +\n    facet_grid(Source~GA) + \n    theme_Publication() +\n    theme(legend.position=\"\",\n          axis.text.x = element_text(angle = 45, hjust=1, size=rel(.8)), \n                axis.title.x = element_text(hjust = 1), # align the title to the right\n          panel.border = element_rect(colour = \"black\"))\n\n\n\n\n6.4.3 Figure 4C\n\n\nCode\n  dt.qux&lt;-dt.best.result[Source==\"Munchel\",.(Source,fold,predictor,AUC=AUC_test/100,AUC_lo=AUC_test_lo/100,AUC_hi=AUC_test_hi/100)]\n  p4.top.model.munchel&lt;-ggplot(dt.qux, aes(predictor, AUC)) + \n    geom_pointrange(aes(ymin=AUC_lo, ymax=AUC_hi),col=\"grey30\",size=.9,alpha=.8) +\n    scale_x_discrete(limits=dt.baz[,.N,predictor]$predictor) +\n    scale_y_continuous(expand=c(0,0),breaks=c(0,.2,.4,.6,.8,1), limit=c(0,1.05)) +\n    ylab(\"AUC\") + xlab(\"\") +\n    facet_grid(Source~.) + #,nrow=1) +\n    theme_Publication() +\n    theme(axis.text.x = element_text(angle = 45, hjust=1, size=rel(.8)), \n          panel.border = element_rect(colour = \"black\"))\n\n\n\n\n6.4.4 Figure 4D\n\n\nCode\n  # dt.roc.data from WF2.extra.bits2.Rmd\n  lazyLoad(\"WF2.extra.bits2_cache/beamer/roc_lep_pappa2_3110fa4c16ccfa47c784596b3531122d\")\n  p4.roc.curve&lt;-ggplot(dt.roc.data, aes(1-specificity,sensitivity)) +\n    geom_rect(data = dt.roc.data[dataset==\"Discovery (28wk)\"], aes(xmin = -Inf,xmax = Inf, ymin = -Inf,ymax = Inf),fill=\"grey90\",alpha = 0.1) +\n    geom_line() +\n    geom_text(data=dt.roc,size=4,\n              mapping=aes(x=.55,y=.5,label=paste0(\"AUC=\",round(roc,3),\" (\",round(roc_lo,3),\", \",round(roc_hi,3),\")\"))\n              ) +\n    ylab(\"Sensitivity\") + xlab(\"1-Specificity\") +\n    facet_grid(geneName~dataset) +\n    theme_Publication() + \n    theme(legend.position=\"\",\n          axis.text.x = element_text(angle = 45, hjust=1), \n          panel.border = element_rect(colour = \"black\"))\n\n\n\n\n6.4.5 Figure 4A-D\n\n\nCode\n  p4.middle.right&lt;-cowplot::plot_grid(NULL,p4.top.model.munchel,labels=c(\"\",\"C\"),ncol=1,label_size=27,rel_heights=c(1,1.5))\n  p4.middle&lt;-cowplot::plot_grid(p4.top.model, p4.middle.right, labels=c(\"B\",\"\"),label_size=27,rel_widths=c(2.6,1))\n\n  pdf(file=\"Figures/cfRNA.Fig4.pdf\", width=11, height=17, title=\"Fig4: model selection and validation\")\n  cowplot::plot_grid(p4.enet.lasso.val3, p4.middle, p4.roc.curve, nrow=3,labels=c(\"A\",\"\",\"D\"),label_size=27,rel_heights=c(1,1.5,1.3), align=\"v\", axis=\"b\")\n  dev.off()",
    "crumbs": [
      "Figures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Main Figures</span>"
    ]
  },
  {
    "objectID": "main_figure.html#sec-fig5",
    "href": "main_figure.html#sec-fig5",
    "title": "6  Main Figures",
    "section": "6.5 Figure 5",
    "text": "6.5 Figure 5\n{$#fig-fig5}\n\n\nCode\n  load(\"RData/dt.cpmZ.preterm.POPS-2022.GRCh38.88.RData\") # dt.cpmZ (preterm)\n  load(\"RData/dt.cpmZ.term.POPS-2022.GRCh38.88.RData\") # dt.cpmZ.term (term)\n  load(\"RData/dt.cpmZ.munchel.RData\") # dt.cpmZ.munchel (Munchel)\n\n  dt.foo&lt;-rbind(\n    dt.cpmZ[geneName %in% c(\"LEP\",\"PAPPA2\"),.(dataset=\"Discovery\",GA,Condition,geneName,logCPM,logCPMZ)],\n    dt.cpmZ.term[geneName %in% c(\"LEP\",\"PAPPA2\"),.(dataset=\"Validation\",GA,Condition,geneName,logCPM,logCPMZ)],\n    dt.cpmZ.munchel[geneName %in% c(\"LEP\",\"PAPPA2\"),.(dataset=\"Munchel\",GA=\"31wk\",Condition,geneName,logCPM,logCPMZ)]\n    )\n  dt.foo[,CPM:=2^logCPM]\n  dt.foo$Condition&lt;-factor(dt.foo$Condition, level=c(\"Case\",\"Control\"))\n  dt.foo$dataset&lt;-factor(dt.foo$dataset, level=c(\"Discovery\",\"Validation\",\"Munchel\"))\n\n  #\n  dt.foo[,summary(.SD),.(dataset,geneName),.SDcols=\"CPM\"]\n  dt.foo$CPM %&gt;% summary\n\n  #######################################\n  # get the p-values (based on logCPMZ) #\n  #######################################\n  # by outcome\n  dt.pval&lt;-dt.foo[,.(Pval=wilcox.test(logCPM~Condition)$p.value),.(dataset,GA,geneName)]\n  dt.pval[,Condition:=\"Case\"] # ggsignif complains without this\n\n  # by GA\n  dt.pval28.36&lt;-dt.foo[GA %in% c(\"28wk\",\"36wk\") & dataset==\"Validation\",.(Pval=wilcox.test(logCPM~GA)$p.value),.(dataset,geneName)][,`:=` (`GA1`=\"28wk\",GA2=\"36wk\")]\n  dt.pval28.36[,Condition:=\"Case\"] # ggsignif complains without this\n\n  dt.pval20.28&lt;-dt.foo[GA %in% c(\"20wk\",\"28wk\"),.(Pval=wilcox.test(logCPM~GA)$p.value),.(dataset,geneName)][,`:=` (`GA1`=\"20wk\",GA2=\"28wk\")]\n  dt.pval20.28[,Condition:=\"Case\"] # ggsignif complains without this\n\n  dt.pval12.20&lt;-dt.foo[GA %in% c(\"12wk\",\"20wk\"),.(Pval=wilcox.test(logCPM~GA)$p.value),.(dataset,geneName)][,`:=` (`GA1`=\"12wk\",GA2=\"20wk\")]\n  dt.pval12.20[,Condition:=\"Case\"] # ggsignif complains without this\n\n  dt.foo$Condition&lt;-relevel(dt.foo$Condition, ref=\"Control\")\n\n  #\n  #\n  p5.lep.pappa2.sig&lt;- ggplot(dt.foo, aes(GA, CPM, col=Condition)) +\n    geom_boxplot(outlier.shape=NA,width=.7,size=.5,alpha=.8) +\n    geom_point(pch = 21, size=2.5, position = position_jitterdodge(), alpha=.7) +\n    scale_y_continuous(trans = scales::log2_trans(),\n                       breaks = scales::trans_breaks(\"log2\",n=5, function(x) 2^x),\n                       labels = scales::trans_format(\"log2\", scales::math_format(2^.x)),\n                        limits=c(min(dt.foo$CPM),max(dt.foo$CPM)*10)) +\n    annotation_logticks(base=2,sides=\"l\") +\n    ggsci::scale_color_jama() +\n    ggsignif::geom_signif(data=dt.pval, \n                          aes(xmin = GA, xmax = GA, annotations = format(Pval, scientific=T,digits=2), y_position = 10),\n                          textsize = 5, vjust = -0.2, tip_length=0.01,\n                          manual = TRUE, color=\"black\") +\n    ggsignif::geom_signif(data=dt.pval28.36, \n                          aes(xmin = GA1, xmax = GA2, annotations = format(Pval, scientific=T,digits=2), y_position = 8.5),\n                          textsize = 5, vjust = -0.2, tip_length=0.03,\n                          manual = TRUE, color=\"black\") +\n    ggsignif::geom_signif(data=dt.pval20.28, \n                          aes(xmin = GA1, xmax = GA2, annotations = format(Pval, scientific=T,digits=2), y_position = 7.5),\n                          textsize = 5, vjust = -0.2, tip_length=0.03,\n                          manual = TRUE, color=\"black\") +\n    ggsignif::geom_signif(data=dt.pval12.20, \n                          aes(xmin = GA1, xmax = GA2, annotations = format(Pval, scientific=T,digits=2), y_position = 6.5),\n                          textsize = 5, vjust = -0.2, tip_length=0.03,\n                          manual = TRUE, color=\"black\") +\n    ylab(\"Count Per Million\") + xlab(\"Gestational Age\") +\n    facet_grid(geneName~dataset, scales=\"free_x\", space=\"free_x\") +\n    theme_Publication() +theme (panel.border = element_rect(colour = \"black\"),legend.position=\"top\")\n\n  #\n  pdf(file=\"Figures/cfRNA.Fig5.sig.pdf\", width=11, height=8.5, title=\"Fig5: Abundance of LEP and PAPPA2\")\n  p5.lep.pappa2.sig\n  dev.off()",
    "crumbs": [
      "Figures",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Main Figures</span>"
    ]
  },
  {
    "objectID": "suppl_figure.html",
    "href": "suppl_figure.html",
    "title": "7  Supplementary Text Figures",
    "section": "",
    "text": "7.1 Supplementary Text Figure A\nCode\n  lazyLoad(\"downsample.chrY.TPM_cache/beamer/load_ds_salmon_4b8dd727729fd0ae5deb423d69e7c16d\") # dl.chrY.TPM and dl.stat\n\n  dt.ds.chrY &lt;- lapply(names(dl.stat), function(DS) dl.stat[[DS]][,DS:=DS]) %&gt;% rbindlist\n  dt.ds.chrY$DS&lt;-factor(dt.ds.chrY$DS, level=c(\"1M\",\"5M\",\"10M\"))\n  dt.ds.chrY[,GA:=gsub(\" weeks\",\"wk\",GA)]\n  setnames(dt.ds.chrY, \"Type\", \"Method\")\n\n  p1.chrY&lt;-ggplot(dt.ds.chrY, aes(Method,SumCount)) +\n  geom_rect(data = dt.ds.chrY[DS==\"5M\"],fill=\"grey60\",xmin = -Inf,xmax = Inf, ymin = -Inf,ymax = Inf,alpha = 0.2) +\n  geom_bar(aes(fill=Method), stat=\"identity\",width=.5, alpha=.7) +\n  ggsci::scale_fill_jama() +\n  facet_grid(Sex~DS+GA,scales=\"free\") +\n  labs(y=\"Sum of read count on chrY genes\") +\n  theme_Publication() +\n  theme(legend.position=\"top\",\n        panel.border=element_rect(colour=\"black\"),\n        axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n  #\n  pdf(file=\"Figures/Suppl/cfRNA.Suppl.Text.Fig1A.pdf\", width=11, height=8, title=\"Abundance of chrY\")\n  p1.chrY\n  dev.off()",
    "crumbs": [
      "Figures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Supplementary Text Figures</span>"
    ]
  },
  {
    "objectID": "suppl_figure.html#sec-si-figure-b",
    "href": "suppl_figure.html#sec-si-figure-b",
    "title": "7  Supplementary Text Figures",
    "section": "7.2 Supplementary Text Figure B",
    "text": "7.2 Supplementary Text Figure B\n\n\n\nCode\n  lazyLoad(\"downsample.chrY.TPM_cache/beamer/load_ds_salmon_4b8dd727729fd0ae5deb423d69e7c16d\") # dl.chrY.TPM and dl.stat\n\n  lapply(dl.chrY.TPM, function(i) i[,.(sum(Counts),sum(TPM))])\n\n  dl.chrY.TPM&lt;-lapply(dl.chrY.TPM, function(DT) {\n                  #DT[Sex==\"Male\",Class:=ifelse(TPM&gt;0,\"TP\",\"FN\")]\n                  #DT[Sex==\"Female\",Class:=ifelse(TPM==0,\"TN\",\"FP\")] \n                  DT[,`Predicted Sex`:=ifelse(TPM&gt;0, \"Male\",\"Female\"),Type]\n                  DT$Sex&lt;-factor(DT$Sex, level=c(\"Male\",\"Female\"))\n                  DT$`Predicted Sex`&lt;-factor(DT$`Predicted Sex`, level=c(\"Male\",\"Female\"))\n                  DT \n  }) \n\n  #####################################\n  # Predictive performance by DS & GA #\n  #####################################\n  dt.chrY.con.ga&lt;-lapply(names(dl.chrY.TPM), function(DS){\n    lapply(split(dl.chrY.TPM[[DS]], dl.chrY.TPM[[DS]]$GA), function(DT){\n    my.GA&lt;-DT[,.N,GA]$GA\n\n    cm1&lt;-caret::confusionMatrix(DT[Type==\"HiSat2+Salmon\"][[\"Predicted Sex\"]], reference=DT[Type==\"HiSat2+Salmon\"][[\"Sex\"]])\n    cm2&lt;-caret::confusionMatrix(DT[Type==\"Salmon (SA mode)\"][[\"Predicted Sex\"]], reference=DT[Type==\"Salmon (SA mode)\"][[\"Sex\"]])\n\n    rbind(\n    data.table(DS=DS, GA=my.GA,Type=\"HiSat2+Salmon\", Measure=names(cm1$byClass), Value=cm1$byClass),\n    data.table(DS=DS, GA=my.GA,Type=\"Salmon (SA mode)\", Measure=names(cm2$byClass), Value=cm2$byClass)\n    )[Measure %in% c(\"F1\",\"Precision\", \"Recall\")]\n    }) %&gt;% rbindlist \n  }) %&gt;% rbindlist \n\n  dt.chrY.con.ga$DS&lt;-factor(dt.chrY.con.ga$DS, level=c(\"1M\",\"5M\",\"10M\"))\n  dt.chrY.con.ga[,GA:=gsub(\" weeks\",\"wk\",GA)]\n  setnames(dt.chrY.con.ga, \"Type\", \"Method\")\n\n  p1.chrY.con&lt;-ggplot(dt.chrY.con.ga, aes(Method,Value)) +\n  geom_rect(data = dt.chrY.con.ga[DS==\"5M\"],fill=\"grey60\",xmin = -Inf,xmax = Inf, ymin = -Inf,ymax = Inf,alpha = 0.2) +\n  geom_bar(aes(fill=Method), stat=\"identity\",width=.5, alpha=.7) +\n  ggsci::scale_fill_jama() +\n  facet_grid(Measure~DS+GA,scales=\"free\") +\n  labs(y=\"Score\") +\n  theme_Publication() +\n  theme(legend.position=\"none\",\n        panel.border=element_rect(colour=\"black\"),\n        axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())\n\n\n  # merge SI.Fig1A + SI.Fig1B\n  pdf(file=\"Figures/Suppl/cfRNA.Suppl.Text.Fig1AB.pdf\", width=11, height=14, title=\"Abundance of chrY\")\n  cowplot::plot_grid(p1.chrY, p1.chrY.con, labels=\"AUTO\", ncol=1, label_size=27, align=\"v\", axis=\"l\")\n  dev.off()",
    "crumbs": [
      "Figures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Supplementary Text Figures</span>"
    ]
  },
  {
    "objectID": "suppl_figure.html#sec-si-figure-c",
    "href": "suppl_figure.html#sec-si-figure-c",
    "title": "7  Supplementary Text Figures",
    "section": "7.3 Supplementary Text Figure C",
    "text": "7.3 Supplementary Text Figure C\n\n\n\nCode\n  # load the transcript definition used in Salmon (SA)\n  load(\"RData/gr.ensg.Homo_sapiens.GRCh38.88.cdna.all.ncrna.fa.gz.RData\") %&gt;% system.time # isa GRanges\n  gr.ensg[seqnames(gr.ensg)==\"Y\" & gr.ensg$gene_biotype==\"protein_coding\",] # chrY protein-coding; n=42 \n  this.gene&lt;-(gr.ensg[seqnames(gr.ensg)==\"Y\" & gr.ensg$gene_biotype==\"protein_coding\",] %&gt;% names) # chrY protein-coding; n=42 (without the training version e.g. ENSG00000176679)\n\n  # load the TPM based om 10M-ds dataset\n  load(\"RData/li.TPM.10M.RData\")\n  li.TPM[[\"Salmon_aln\"]][[\"TPM\"]] %&gt;% dim # 62803 genes x 100 samples\n\n  ## Heatamp of TPM using `HiSat2+Salmon` (based on 10M)\n  li.TPM[[\"Salmon_aln\"]][[\"counts\"]][this.gene,] %&gt;% rowSums %&gt;% sort %&gt;% rev # count of 42 chrY genes\n  filter.g2&lt;-li.TPM[[\"Salmon_aln\"]][[\"counts\"]][this.gene,] %&gt;% rowSums %&gt;% sort %&gt;% rev &gt;0 # filter for 42 chrY genes with TPM &gt;0\n  this.gene2&lt;-(li.TPM[[\"Salmon_aln\"]][[\"counts\"]][this.gene,] %&gt;% rowSums %&gt;% sort %&gt;% rev)[filter.g2] %&gt;% names # n=24 genes\n  this.sample2&lt;-li.TPM[[\"Salmon_aln\"]][[\"counts\"]][this.gene,] %&gt;% colSums %&gt;% sort %&gt;% rev %&gt;% names # n=100 samples sorted \n  filter.s2&lt;-li.TPM[[\"Salmon_aln\"]][[\"counts\"]][this.gene2,] %&gt;% colSums %&gt;% sort %&gt;% rev &gt;0 # filter for samples with TPM &gt;0 \n  predicted.male2&lt;-(li.TPM[[\"Salmon_aln\"]][[\"counts\"]][this.gene,] %&gt;% colSums %&gt;% sort %&gt;% rev)[filter.s2] %&gt;% names # n=60 samples \n\n  my.mat2&lt;-log2(li.TPM[[\"Salmon_aln\"]][[\"counts\"]][this.gene2,this.sample2]+1)\n  rownames(my.mat2)&lt;-gr.ensg[this.gene2]$gene_name\n\n  col.anno&lt;-mat.samples[this.sample2,c(\"GA\",\"Sex\"),drop=F] # isa `data.frame`\n  col.anno$\"Predicted Sex\"&lt;-ifelse(this.sample2 %in% predicted.male2,\"Male\",\"Female\")\n  col.anno&lt;-col.anno[,c(\"Predicted Sex\",\"Sex\",\"GA\")]\n\n  GA.color&lt;-ggsci::pal_jama(\"default\",alpha=.9)(4)\n  names(GA.color)&lt;-c(\"12wk\",\"20wk\",\"28wk\",\"36wk\")\n  my.color=list(\n      `Sex`=c(Female=\"hotpink\",Male='skyblue'),\n      `Predicted Sex`=c(Female=\"hotpink3\",Male='skyblue3'),\n      #`GA`=c(\"12wk\"=cbPalette2[1],\"20wk\"=cbPalette2[2],\"28wk\"=cbPalette2[3],\"36wk\"=cbPalette2[4])\n      `GA`=GA.color\n  )\n\n  cph1&lt;-ComplexHeatmap::pheatmap(my.mat2,\n                                 annotation_col=col.anno, \n                                 annotation_colors=my.color, \n                                 cluster_cols=F,\n                                 cluster_rows=F,\n                                 show_colnames=F,\n                                 border_color=NA,\n                                 fontsize=12,\n                                 name=\"log2(count)\",\n                                 row_title=\"\\nHiSat2+Salmon\",\n                                 row_title_gp = gpar(fontsize = 20),\n  )\n\n  filter.g3&lt;-li.TPM[[\"Salmon\"]][[\"counts\"]][this.gene,] %&gt;% rowSums %&gt;% sort %&gt;% rev &gt;0 # filter for 42 chrY genes with count &gt;0\n  this.gene3&lt;-(li.TPM[[\"Salmon\"]][[\"counts\"]][this.gene,] %&gt;% rowSums %&gt;% sort %&gt;% rev)[filter.g3] %&gt;% names # n=16 genes (out of 42) &gt;0 count \n  this.sample3&lt;-li.TPM[[\"Salmon\"]][[\"counts\"]][this.gene,] %&gt;% colSums %&gt;% sort %&gt;% rev %&gt;% names # 100 sample names\n  filter.s3&lt;-li.TPM[[\"Salmon\"]][[\"counts\"]][this.gene3,] %&gt;% colSums %&gt;% sort %&gt;% rev &gt;0 # samples with count&gt;0 \n  #(li.TPM[[\"Salmon\"]][[\"counts\"]][this.gene,] %&gt;% colSums %&gt;% sort %&gt;% rev)[filter.s3] # n=38 samples with count&gt;0 (predicted as Male)\n  predicted.male3&lt;-(li.TPM[[\"Salmon\"]][[\"counts\"]][this.gene,] %&gt;% colSums %&gt;% sort %&gt;% rev)[filter.s3] %&gt;% names # n=38 samples with count&gt;0 (predicted as Male)\n\n  my.mat3&lt;-log2(li.TPM[[\"Salmon\"]][[\"counts\"]][this.gene3,this.sample3]+1)\n  rownames(my.mat3)&lt;-gr.ensg[this.gene3]$gene_name\n\n  col.anno3&lt;-mat.samples[this.sample3,c(\"GA\",\"Sex\"),drop=F]\n  col.anno3$\"Predicted Sex\"&lt;-ifelse(this.sample3 %in% predicted.male3,\"Male\",\"Female\")\n  col.anno3&lt;-col.anno3[,c(\"Predicted Sex\",\"Sex\",\"GA\")]\n\n  # via ComplexHeatmap\n  cph3&lt;-ComplexHeatmap::pheatmap(my.mat3,\n                                 annotation_col=col.anno3, \n                                 annotation_colors=my.color, \n                                 cluster_cols=F,\n                                 cluster_rows=F,\n                                 show_colnames=F,\n                                 border_color=NA,\n                                 fontsize=12,\n                                 name=\"log2(count)\",\n                                 row_title=\"\\nSalmon (SA mode)\",\n                                 row_title_gp = gpar(fontsize = 20),\n                                 heatmap_legend_param = list(\n                                                            title_gp=gpar(fontsize=13,fontface=\"bold\"),\n                                                            labels_gp=gpar(fontsize=11),\n                            annotation_legend_param = list(title_gp=gpar(fontsize=13,fontface=\"bold\"),labels_gp = gpar(fontsize = 11))\n                                )\n  )\n\n\n  pdf(file=\"Figures/Suppl/cfRNA.Suppl.Text.Fig1C.pdf\", width=11, height=14, title=\"Abundance of chrY\")\n  draw(cph1 %v% cph3, main_heatmap=\"log2(count)\", ht_gap=unit(.7,\"cm\"), auto_adjust=F)\n  grid.text(label=\"C\",gp=gpar(fontsize=27,fontface=\"bold\"),x=0.02,y=.99,just=\"top\")\n  dev.off()",
    "crumbs": [
      "Figures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Supplementary Text Figures</span>"
    ]
  },
  {
    "objectID": "suppl_figure.html#sec-si-figure-d",
    "href": "suppl_figure.html#sec-si-figure-d",
    "title": "7  Supplementary Text Figures",
    "section": "7.4 Supplementary Text Figure D",
    "text": "7.4 Supplementary Text Figure D\n\n\n\nCode\n  dl.chrY.TPM %&gt;% names\n\n  # load the TPM based om 10M-ds dataset\n  load(\"RData/li.TPM.10M.trim.RData\")\n  li.TPM[[\"Salmon\"]][[\"TPM\"]] %&gt;% dim # 59354 genes x 100 samples\n\n  dt.trim.10M&lt;-data.table(\n      `Type`=\"Salmon (SA mode)\",\n      `SampleID`=li.TPM[[\"Salmon\"]][[\"counts\"]][this.gene,] %&gt;% colSums %&gt;% names,\n      `Counts`=li.TPM[[\"Salmon\"]][[\"counts\"]][this.gene,] %&gt;% colSums, \n      `TPM`=li.TPM[[\"Salmon\"]][[\"TPM\"]][this.gene,] %&gt;% colSums \n      )\n  # add Sex and GA\n  dt.trim.10M&lt;-merge(dt.trim.10M, dt.samples[,.(SampleID,GA,Sex,pn_female)])\n\n  dt.trim.10M[,`Predicted Sex`:=ifelse(TPM&gt;0, \"Male\",\"Female\"),Type]\n  dt.trim.10M$Sex&lt;-factor(dt.trim.10M$Sex, level=c(\"Male\",\"Female\"))\n  dt.trim.10M$`Predicted Sex`&lt;-factor(dt.trim.10M$`Predicted Sex`, level=c(\"Male\",\"Female\"))\n\n  dt.chrY.trim&lt;-rbind(\n        dl.chrY.TPM[[\"10M\"]][Type==\"Salmon (SA mode)\"][,Trim:=\"No\"],\n        dt.trim.10M[,Trim:=\"Yes\"]\n        )\n  ## Number of false positives: chrY signal detected from females \n  dt.chrY.trim[,.(Trim,Sex,`Predicted Sex`)] %&gt;% ftable\n\n  ##\n  ## Read counts by trim or not\n  dt.bar&lt;-merge(\n    dl.chrY.TPM[[\"10M\"]][Type==\"Salmon (SA mode)\",.(SampleID,GA=gsub(\" weeks\",\"wk\",GA),Sex,Counts,TPM)], # no-trim\n    dt.trim.10M[Type==\"Salmon (SA mode)\",.(SampleID,GA,Sex,Counts,TPM)], # trim\n    by=c(\"SampleID\",\"Sex\",\"GA\")\n  )\n  cor.test(~Counts.x + Counts.y, dt.bar)\n  dt.bar[Counts.x&gt;Counts.y] # no-trim &gt; trim\n  dt.bar[Counts.x&lt;Counts.y] # no-trim &lt; trim\n  dt.bar[Sex==\"Female\" & Counts.x&lt;Counts.y] # more counts for Trimmed Salmon\n\n  my.limit&lt;-dt.bar[,max(Counts.x,Counts.y)]  %&gt;% ceiling\n  p1.chrY.trim&lt;-ggplot(dt.bar, aes(Counts.x+1,Counts.y+1)) + \n  geom_point(data=dt.bar, size=5, position=position_jitter(width=0.2,height=0.2),shape=21) +\n  scale_x_continuous(trans = scales::log2_trans(),\n                     breaks = scales::trans_breaks(\"log2\",function(x) 2^x),\n                     labels = scales::trans_format(\"log2\", math_format(2^.x)),\n                     limits=c(.5,my.limit*2)) +\n  scale_y_continuous(trans = scales::log2_trans(),\n                     breaks = scales::trans_breaks(\"log2\",function(x) 2^x),\n                     labels = scales::trans_format(\"log2\", math_format(2^.x)),\n                     limits=c(.5,my.limit*2)) +\n  #coord_fixed() + \n  facet_grid(Sex~GA,scales=\"free_x\") +\n  labs(x=\"Sum of read count (No trimmed reads)\", y=\"Sum of read count (trimmed reads)\") +\n  theme_Publication() + theme(panel.border = element_rect(colour = \"black\"))\n\n  pdf(file=\"Figures/Suppl/cfRNA.Suppl.Text.Fig1D.v2.pdf\", width=11, height=7, title=\"Trim vs No trim\")\n  cowplot::plot_grid(p1.chrY.trim, labels=c(\"D\"),label_size=27)\n  dev.off()",
    "crumbs": [
      "Figures",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Supplementary Text Figures</span>"
    ]
  },
  {
    "objectID": "figure.html",
    "href": "figure.html",
    "title": "Figures",
    "section": "",
    "text": "This chapter demonstrates the R and the dot codes that make 6  Main Figures and 7  Supplementary Text Figures shown in the paper.\n\n\n\n\n\n\nR package dependencies\n\n\n\nYou need to install R ggplot2 as well as my own publication theme called theme_Publication in _libs.\n\n\nCode\nlibrary(\"ggplot2\")\nsource(\"static/R/_libs/theme_publish.R\")",
    "crumbs": [
      "Figures"
    ]
  }
]